{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openpyxl\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.cluster import BaseCluster\n",
    "import re\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from cuml.manifold import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from cuml.cluster import HDBSCAN\n",
    "from hdbscan.flat import (HDBSCAN_flat,\n",
    "                          approximate_predict_flat,\n",
    "                          membership_vector_flat,\n",
    "                          all_points_membership_vectors_flat)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "#from pycave.bayes import GaussianMixture as GMM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi\n",
    "# Check available GPUs\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X)\n",
    "        self.labels_ = self.model.predict(X)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        self.labels_ = predictions\n",
    "        return predictions\n",
    "    \n",
    "# class MyGMM:\n",
    "#     def __init__(self, num_components, trainer_params):\n",
    "#         self.gmm = GMM(num_components=num_components,trainer_params=trainer_params)\n",
    "#         self.labels_ = None\n",
    "    \n",
    "#     def fit(self,data):\n",
    "#         self.gmm.fit(data)\n",
    "#         self.labels_ = np.array(self.gmm.predict(data))\n",
    "#         return self\n",
    "    \n",
    "#     def predict(self,data):\n",
    "#         return np.array(self.gmm.predict(data))\n",
    "\n",
    "class Dimensionality:\n",
    "  \"\"\" Use this for pre-calculated reduced embeddings \"\"\"\n",
    "  def __init__(self, reduced_embeddings):\n",
    "    self.reduced_embeddings = reduced_embeddings\n",
    "\n",
    "  def fit(self, X):\n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    return self.reduced_embeddings\n",
    "\n",
    "# Remove individual commenter first\n",
    "def remove_individual(headline):\n",
    "    by_pattern = re.search(r\" (-+)? By\", headline, flags = re.IGNORECASE)\n",
    "    if by_pattern:\n",
    "        return headline[:by_pattern.start()]\n",
    "    else:\n",
    "        return headline\n",
    "    \n",
    "# Tokenize the headline into tokens which are alphanumeric words including period . \n",
    "def custom_tokenizer(headline):\n",
    "    tokens = re.findall(r\"\\b[a-zA-z0-9\\.][a-zA-z0-9\\.]+\\b\",headline.lower())  \n",
    "    return tokens\n",
    "\n",
    "# Remove tokens according to the principles \n",
    "def custom_processor(headline, remove_words):\n",
    "    tokens = custom_tokenizer(headline)\n",
    "    new_tokens = [token for token in tokens if token not in remove_words]\n",
    "    return \" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_dict = {}\n",
    "r2_tr_dict = {}\n",
    "r2_te_dict = {}\n",
    "cluster_num = 60\n",
    "hdb_min_cluster_size = 1400\n",
    "df_folder = '/shared/share_tm-finance'\n",
    "datatype = 'contem'\n",
    "modeltype = 'pcakmeans'\n",
    "save_model = True\n",
    "for i in range(2023, 2024):\n",
    "    tar_year = i\n",
    "    \n",
    "    df = pd.read_csv(df_folder+'/Processed_df/One_year_window/{type}_{year}.csv'.format(year = tar_year, type = datatype))\n",
    "\n",
    "    # Load embeddings\n",
    "    red_headlines = df.vocab_con_headline.tolist()\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = np.load(df_folder+\"/Embeddings/One_year_window/{type}_{year}_embeddings.npy\".format(year = tar_year, type = datatype))\n",
    "\n",
    "    # Reduce dimensionality\n",
    "    PCA_model = PCA(n_components=10)\n",
    "    SVD_model = TruncatedSVD(n_components=10, random_state=42, n_iter=10) \n",
    "    umap_model = UMAP(n_neighbors=10, n_components=10, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    # Cluster embeddings\n",
    "    gmm_model = ClusterModel(GaussianMixture(n_components=100, covariance_type='full', random_state = 42))\n",
    "    # gmm_model2 = MyGMM(num_components=cluster_num,trainer_params={\"accelerator\":'gpu',\"devices\":1})\n",
    "    KMeans_model = KMeans(n_clusters=cluster_num)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size = hdb_min_cluster_size,  metric='euclidean', cluster_selection_method='eom',\\\n",
    "                             gen_min_span_tree=True,prediction_data=False,min_samples = 50,verbose = True)\n",
    "    # Vectorize\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=1, max_df = 50, ngram_range=(1, 2))\n",
    "    \n",
    "    model_combinations = {\n",
    "    'umaphdbscan': (umap_model, hdbscan_model),\n",
    "    'pcakmeans': (PCA_model, KMeans_model),\n",
    "    'umapgmm': (umap_model, gmm_model),\n",
    "    'pcagmm': (PCA_model, gmm_model),\n",
    "    'svdkeans': (SVD_model, KMeans_model)\n",
    "    }\n",
    "\n",
    "    dim_red_model, cluster_model = model_combinations.get(modeltype, (PCA_model, KMeans_model))\n",
    "        \n",
    "\n",
    "    if modeltype == 'umaphdbscan_2':\n",
    "        while True:\n",
    "            reducer = UMAP(n_neighbors=10, n_components=10, min_dist=0.0, metric='cosine', random_state=42, n_epochs=1000, learning_rate=0.5)\n",
    "            reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "            clusterer = HDBSCAN(min_cluster_size = hdb_min_cluster_size,  metric='euclidean', cluster_selection_method='eom',\\\n",
    "                                gen_min_span_tree=True,prediction_data=False,min_samples = 50,verbose = True).fit(reduced_embeddings)\n",
    "            if len(set(clusterer.labels_)) >= cluster_num - 5 and len(set(clusterer.labels_)) <= cluster_num + 5:\n",
    "                break\n",
    "        dim_red_model = Dimensionality(reduced_embeddings)\n",
    "\n",
    "    Topic_model = BERTopic(embedding_model=embedding_model, umap_model=dim_red_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model,\n",
    "                        calculate_probabilities = False, verbose = True, low_memory = True)\n",
    "\n",
    "    r2 = []\n",
    "    r2_tr = []\n",
    "    r2_te = []\n",
    "    m = np.random.randint(0,5)\n",
    "    for i in range(1):\n",
    "        \n",
    "        if modeltype == 'umaphdbscan_1':\n",
    "            reducer = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "            reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "            clusterer = HDBSCAN_flat(X = reduced_embeddings, cluster_selection_method='eom', metric='euclidean', n_clusters=60, min_cluster_size=500, \\\n",
    "                            gen_min_span_tree=True, prediction_data=False, min_samples = 50)\n",
    "            dim_red_model = Dimensionality(reduced_embeddings)\n",
    "            cluster_model = BaseCluster()\n",
    "            Topic_model = BERTopic(embedding_model=embedding_model, umap_model=dim_red_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model,\n",
    "                        calculate_probabilities = False, verbose = True, low_memory = True)\n",
    "        \n",
    "        topics, _ = Topic_model.fit_transform(red_headlines, embeddings)\n",
    "        topic_dist, _ = Topic_model.approximate_distribution(red_headlines)\n",
    "        if i == m:\n",
    "            #save the topic model\n",
    "            if save_model == True:\n",
    "                Topic_model.save(df_folder+'/Kevin/Bert_var/One_year_window/{model}/{year}_{topic_num}_{count}'.format(year = tar_year, topic_num = cluster_num, model = modeltype, count = m+1),\n",
    "                serialization = \"safetensors\", save_ctfidf = True, save_embedding_model = embedding_model)\n",
    "            \n",
    "        contem_ret_topic_dist = pd.concat([df.drop(columns = [\"rp_entity_id\",\"headline\",\"vocab_con_headline\"]),pd.DataFrame(topic_dist)],axis = 1)\n",
    "        grouped = contem_ret_topic_dist.groupby(['date',\"comnam\",\"ret\"])\n",
    "        grouped_sum = grouped.sum()\n",
    "\n",
    "        X = np.array(grouped_sum)\n",
    "        ret = [ind[2] for ind in list(grouped_sum.index)]\n",
    "        Y = np.array(ret).reshape(-1,1)\n",
    "        X_tr, X_te, Y_tr, Y_te = train_test_split(X,Y,test_size=0.2,random_state=66)\n",
    "        \n",
    "        sample_regression = LinearRegression(fit_intercept=True).fit(X_tr,Y_tr)\n",
    "        R_square_tr = sample_regression.score(X_tr,Y_tr)\n",
    "        R_square_te = sample_regression.score(X_te,Y_te)\n",
    "        # Y_tr_pred = sample_regression.predict(X_tr)\n",
    "        # Y_te_pred = sample_regression.predict(X_te)\n",
    "        # mse_tr = mean_squared_error(Y_tr,Y_tr_pred)\n",
    "        # mse_te= mean_squared_error(Y_te,Y_te_pred)\n",
    "        full_regression = LinearRegression(fit_intercept=True).fit(X,Y)\n",
    "        R_square = full_regression.score(X,Y)\n",
    "        \n",
    "        r2.append(R_square)\n",
    "        r2_tr.append(R_square_tr)\n",
    "        r2_te.append(R_square_te)\n",
    "    r2_dict[tar_year] = r2\n",
    "    r2_tr_dict[tar_year] = r2_tr\n",
    "    r2_te_dict[tar_year] = r2_te\n",
    "    print(\"Year {year} is done\".format(year = tar_year))\n",
    "\n",
    "print(r2_dict)\n",
    "print(r2_tr_dict)\n",
    "print(r2_te_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute model cohenrence score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# topic_model = BERTopic(verbose=True, n_gram_range=(1, 3))\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": red_headlines,\n",
    "                          \"ID\": range(len(red_headlines)),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = Topic_model._preprocess_text(documents_per_topic.Document.values) \n",
    "vectorizer = Topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in Topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute topic coherence score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "import pandas as pd\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": red_headlines,\n",
    "                          \"ID\": range(len(red_headlines)),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = Topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "vectorizer = Topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "\n",
    "coherence_scores = []\n",
    "\n",
    "# Compute coherence score for each topic\n",
    "for topic in range(len(set(topics))-1):\n",
    "    topic_words = [[words for words, _ in Topic_model.get_topic(topic)]]\n",
    "    coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                     texts=tokens, \n",
    "                                     corpus=corpus,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence='c_v')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    print(f\"Topic {topic} Coherence Score: {coherence}\")\n",
    "    coherence_scores.append((topic, coherence))\n",
    "\n",
    "coherence_df = pd.DataFrame(coherence_scores, columns=['Topic', 'Coherence_Score'])\n",
    "print(coherence_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute model diversity score\n",
    "def compute_topic_diversity(topics):\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "    for topic in topics:\n",
    "        words = topic[1]\n",
    "        total_words += len(words)\n",
    "        unique_words.update(words)\n",
    "    diversity_score = len(unique_words) / total_words\n",
    "    return diversity_score\n",
    "\n",
    "model_diversity = compute_topic_diversity(topics)\n",
    "print(f\"Model Diversity: {model_diversity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite Score Calculation\n",
    "composite_score = 0.4 * coherence_score + 0.3 * model_diversity + 0.3 * R_square\n",
    "\n",
    "# Define Thresholds for Filtering Topics\n",
    "coherence_threshold = 0.5\n",
    "diversity_threshold = 0.5\n",
    "impact_threshold = 0.1\n",
    "\n",
    "# Filter Topics\n",
    "if coherence_score > coherence_threshold and model_diversity > diversity_threshold and R_square > impact_threshold:\n",
    "    print(\"Topics are meaningful and relevant.\")\n",
    "else:\n",
    "    print(\"Topics need further refinement.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
