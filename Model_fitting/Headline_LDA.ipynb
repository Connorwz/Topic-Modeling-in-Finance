{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/rl3444/.conda/envs/NLP_Env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import openpyxl\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from lda import LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "# import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "# Check available GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardization(css_sum_by_topic):\n",
    "    css_sum_by_topic_df = css_sum_by_topic.reset_index()\n",
    "    css_sum_by_topic_df.columns = ['topic', 'css']\n",
    "    scaler = StandardScaler()\n",
    "    css_sum_by_topic_df['css_standardized'] = scaler.fit_transform(css_sum_by_topic_df[['css']])\n",
    "    css_standardized_series = css_sum_by_topic_df.set_index('topic')['css_standardized']\n",
    "    return css_standardized_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(tr_grouped_sum, te_grouped_sum, insampler2_list, outsampler2_list):\n",
    "    tr_X = np.array(tr_grouped_sum)\n",
    "    tr_ret = [ind[2] for ind in list(tr_grouped_sum.index)]\n",
    "    tr_Y = np.array(tr_ret).reshape(-1,1)\n",
    "    tr_regression = LinearRegression(fit_intercept=True)\n",
    "    tr_regression.fit(tr_X,tr_Y)\n",
    "    tr_Y_pred = tr_regression.predict(tr_X)\n",
    "    tr_Y_mean = np.mean(tr_Y)\n",
    "    tr_SS_tot = np.sum((tr_Y - tr_Y_mean) ** 2)\n",
    "    tr_SS_res = np.sum((tr_Y - tr_Y_pred) ** 2)\n",
    "    tr_r2 = 1 - (tr_SS_res / tr_SS_tot)\n",
    "    insampler2_list.append(tr_r2)\n",
    "\n",
    "    te_X = np.array(te_grouped_sum)\n",
    "    te_ret = [ind[2] for ind in list(te_grouped_sum.index)]\n",
    "    te_Y = np.array(te_ret).reshape(-1,1)\n",
    "    te_Y_pred = tr_regression.predict(te_X)\n",
    "    te_SS_tot = np.sum((te_Y - tr_Y_mean) ** 2)\n",
    "    te_SS_res = np.sum((te_Y - te_Y_pred) ** 2)\n",
    "    te_r2 = 1 - (te_SS_res / te_SS_tot)\n",
    "    outsampler2_list.append(te_r2)\n",
    "\n",
    "    return\n",
    "\n",
    "def train_model(saved_model_folder, df, red_headlines, topic_num, cluster_num, tar_year, year_num, save_model, i):\n",
    "    # Perform the train-test split on indices\n",
    "    indices = np.arange(len(red_headlines))\n",
    "    tr_ind, te_ind = train_test_split(indices, test_size=0.2, shuffle= True, random_state=i)\n",
    "\n",
    "    # torch.cuda.empty_cache()\n",
    "    # gc.collect()\n",
    "    tr_df = df.iloc[tr_ind,:]\n",
    "    te_df = df.iloc[te_ind,:]\n",
    "    tr_headlines = [red_headlines[ind] for ind in tr_ind]\n",
    "    te_headlines = [red_headlines[ind] for ind in te_ind]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    tr_doc_term = vectorizer.fit_transform(tr_headlines)\n",
    "    te_doc_term = vectorizer.fit_transform(te_headlines)\n",
    "\n",
    "    lda_model = LDA(n_topics = cluster_num, n_iter = 100, random_state = 66)\n",
    "    lda_model.fit(tr_doc_term)\n",
    "\n",
    "    #save the topic model\n",
    "    if save_model == True:\n",
    "        os.makedirs(saved_model_folder+f'/{tar_year - year_num + 1}_{tar_year}_{topic_num}_{i}', exist_ok = True)            \n",
    "        with open(saved_model_folder+f'/{tar_year - year_num + 1}_{tar_year}_{topic_num}_{i}/{tar_year - year_num + 1}_{tar_year}_{topic_num}_{i}_model', 'wb') as file:\n",
    "            pickle.dump(lda_model,file)\n",
    "    \n",
    "    #calculate insample R2\n",
    "    tr_topic_dist = lda_model.doc_topic_\n",
    "    tr_df = tr_df.reset_index(drop = True)\n",
    "    tr_contem_ret_topic_dist = pd.concat([tr_df.drop(columns = [\"rp_entity_id\",\"headline\",\"vocab_con_headline\"]),pd.DataFrame(tr_topic_dist)],axis = 1)\n",
    "    tr_grouped = tr_contem_ret_topic_dist.groupby(['date',\"comnam\",\"ret\"])\n",
    "    tr_grouped_sum = tr_grouped.sum()\n",
    "    data = []\n",
    "    for i, topic_dist in enumerate(tr_topic_dist):\n",
    "        topic = topic_dist.argmax()\n",
    "        data.append({'Document': i, 'Topic': topic, 'Topic Distribution': topic_dist})\n",
    "    Topic_df = pd.DataFrame(data)\n",
    "    topics = Topic_df[\"Topic\"].to_numpy()\n",
    "    \n",
    "    tr_grouped_sum.drop(columns = ['css'], inplace = True)   \n",
    "\n",
    "    #calculate outsample R2\n",
    "    te_topic_dist = lda_model.transform(te_doc_term)\n",
    "    te_df = te_df.reset_index(drop = True)\n",
    "    te_contem_ret_topic_dist = pd.concat([te_df.drop(columns = [\"rp_entity_id\",\"headline\",\"vocab_con_headline\"]),pd.DataFrame(te_topic_dist)],axis = 1)\n",
    "    te_grouped = te_contem_ret_topic_dist.groupby(['date',\"comnam\",\"ret\"])\n",
    "    te_grouped_sum = te_grouped.sum()\n",
    "    new_data = []\n",
    "    for i, topic_dist in enumerate(te_topic_dist):\n",
    "        topic = topic_dist.argmax()\n",
    "        new_data.append({'Document': i, 'Topic': topic, 'Topic Distribution': topic_dist})\n",
    "    new_Topic_df = pd.DataFrame(new_data)\n",
    "    new_topics = new_Topic_df[\"Topic\"].to_numpy()\n",
    "    \n",
    "    te_grouped_sum.drop(columns = ['css'], inplace = True)\n",
    "\n",
    "    return lda_model, tr_grouped_sum, te_grouped_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate Model Diversity Score\n",
    "def compute_model_diversity(topics):\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "    for topic in topics:\n",
    "        total_words += len(topic)\n",
    "        for words in topic:\n",
    "            unique_words.add(words)\n",
    "    diversity_score = len(unique_words) / total_words\n",
    "    return diversity_score\n",
    "\n",
    "\n",
    "def calculate_score(lda_model, vectorizer, embedding_model):\n",
    "    topic_words = {}  # Stores the top words for each topic\n",
    "    topic_words_list = []  # Stores lists of top words for all topics (used for computing model diversity)\n",
    "    # Retrieve the vocabulary from the vectorizer\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Loop through each topic's word distribution in the LDA model\n",
    "    for topic_idx, topic_dist_data in enumerate(lda_model.topic_word_):\n",
    "        top_words_idx = np.argsort(topic_dist_data)[::-1][:10] # Get the indices of the top 10 words for the current topic, sorted by their probability in descending order\n",
    "        top_words = [vocab[i] for i in top_words_idx]\n",
    "        topic_words[topic_idx] = top_words\n",
    "        topic_words_list.append(top_words)\n",
    "    # Initialize a dictionary to store the embeddings of the words for each topic\n",
    "    topic_embeddings = {} # topics * words * embeddings\n",
    "    for topic, words in topic_words.items():\n",
    "        embeddings = embedding_model.encode(words) # Use a pre-trained embedding model to encode the words into vector representations (embeddings)\n",
    "        topic_embeddings[topic] = embeddings\n",
    "\n",
    "\n",
    "    # Calculate the intra-topic similarity (similarity between words within the same topic)\n",
    "    in_topic_similarity = {}  # Dictionary to store intra-topic similarity scores for each topic\n",
    "    # Loop through each topic and its word embeddings\n",
    "    for topic, embeddings in topic_embeddings.items():\n",
    "        distances = 0  # Initialize a variable to accumulate the cosine similarity between word pairs\n",
    "        for i in range(len(embeddings)): # Loop through all pairs of word embeddings within the topic\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                distances += cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))\n",
    "        distances /= (len(embeddings) - 0) * (len(embeddings) - 1) / 2\n",
    "        in_topic_similarity[topic] = distances[0][0]\n",
    "    in_topic_similarity = np.mean(list(in_topic_similarity.values())) # Calculate the mean intra-topic similarity across all topics\n",
    "    # for topic, score in in_topic_similarity.items():\n",
    "    #     print(f\"Topic {topic}: Mean Cosine Distance = {score}\")\n",
    "\n",
    "\n",
    "    # Calculate the between-topic similarity (similarity between different topics)\n",
    "    distances = 0\n",
    "    count = 0\n",
    "    for topic1, topic2 in combinations(topic_words, 2):\n",
    "        # Compute the centroid (average embedding) for each topic by averaging the word embeddings\n",
    "        for i in range(len(topic_embeddings[topic1])):\n",
    "            for j in range(len(topic_embeddings[topic2])):\n",
    "                distances += cosine_similarity(topic_embeddings[topic1][i].reshape(1, -1), topic_embeddings[topic2][j].reshape(1, -1))\n",
    "                count += 1\n",
    "\n",
    "    btn_topic_similarity = distances[0][0] / count\n",
    "\n",
    "\n",
    "    # Compute model diversity\n",
    "    model_diversity = compute_model_diversity(topic_words_list)\n",
    "\n",
    "    return in_topic_similarity, btn_topic_similarity, model_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insampler2_dict = {}\n",
    "outsampler2_dict = {}\n",
    "cos_in_topic_dict = {}\n",
    "cos_bet_topic_dict = {}\n",
    "model_diversity_dict = {}\n",
    "cos_in_topic_dic = {}\n",
    "cos_bet_topic_dic = {}\n",
    "model_diversity_dic = {}\n",
    "\n",
    "modeltype = 'lda'\n",
    "models = 'three_models'\n",
    "combine = True\n",
    "sentiment_type = 'no_senti'\n",
    "topic_num = 120\n",
    "save_model = False\n",
    "datatype = 'contem'\n",
    "year_num = 1\n",
    "year_list = range(2023, 2024)\n",
    "df_folder = f\"/shared/share_tm-finance/Final/Processed_df/{year_num}_year_window\"\n",
    "embeddings_folder = f\"/shared/share_tm-finance/Final/Embeddings/{year_num}_year_window\"\n",
    "saved_model_folder = f\"/shared/share_tm-finance/Final/Stored_model/{year_num}_year_window/one_model/{modeltype}\"\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "first_count = 1\n",
    "last_count = 5\n",
    "div = last_count - first_count + 1\n",
    "\n",
    "for i in year_list:\n",
    "    \n",
    "    tar_year = i\n",
    "    df = pd.read_csv(df_folder+f'/{datatype}_{tar_year - year_num + 1}_{tar_year}.csv')\n",
    "    red_headlines = df.vocab_con_headline.tolist()\n",
    "    embeddings = np.load(embeddings_folder+f\"/{datatype}_{tar_year - year_num + 1}_{tar_year}_embeddings.npy\")\n",
    "    indices = np.arange(len(red_headlines))\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(red_headlines)\n",
    "\n",
    "    pos_insampler2_list = []\n",
    "    pos_outsampler2_list = []\n",
    "    neg_insampler2_list = []\n",
    "    neg_outsampler2_list = []\n",
    "    neu_insampler2_list = []\n",
    "    neu_outsampler2_list = []\n",
    "    insampler2_list = []\n",
    "    outsampler2_list = []\n",
    "    \n",
    "    pos_indices = df[df['css'] > 0].index\n",
    "    neg_indices = df[df['css'] < 0].index\n",
    "    neu_indices = df[df['css'] == 0].index\n",
    "    pos_df = df.iloc[pos_indices,:]\n",
    "    neg_df = df.iloc[neg_indices,:]\n",
    "    neu_df = df.iloc[neu_indices,:]\n",
    "    pos_headlines = [red_headlines[ind] for ind in pos_indices]\n",
    "    neg_headlines = [red_headlines[ind] for ind in neg_indices]\n",
    "    neu_headlines = [red_headlines[ind] for ind in neu_indices]\n",
    "    pos_embeddings = embeddings[pos_indices,:]\n",
    "    neg_embeddings = embeddings[neg_indices,:]\n",
    "    neu_embeddings = embeddings[neu_indices,:]\n",
    "    \n",
    "    cos_in_topic_pos_sum = 0\n",
    "    cos_bet_topic_pos_sum = 0\n",
    "    model_diversity_pos_sum = 0\n",
    "    cos_in_topic_neg_sum = 0\n",
    "    cos_bet_topic_neg_sum = 0\n",
    "    model_diversity_neg_sum = 0\n",
    "    cos_in_topic_neu_sum = 0\n",
    "    cos_bet_topic_neu_sum = 0\n",
    "    model_diversity_neu_sum = 0\n",
    "\n",
    "    #set pos_cluster_num, neg_cluster_num, neu_cluster_num based on the number of embeddings\n",
    "    pos_cluster_num = int(topic_num * len(pos_embeddings) / len(embeddings))\n",
    "    neg_cluster_num = int(topic_num * len(neg_embeddings) / len(embeddings))\n",
    "    neu_cluster_num = int(topic_num * len(neu_embeddings) / len(embeddings))\n",
    "    diff = topic_num - (pos_cluster_num + neg_cluster_num + neu_cluster_num)\n",
    "    if pos_cluster_num < neg_cluster_num and pos_cluster_num < neu_cluster_num:\n",
    "        pos_cluster_num += diff\n",
    "    elif neg_cluster_num < pos_cluster_num and neg_cluster_num < neu_cluster_num:\n",
    "        neg_cluster_num += diff\n",
    "    else:\n",
    "        neu_cluster_num += diff\n",
    "    \n",
    "    cos_in_topic_pos_sum = 0\n",
    "    cos_bet_topic_pos_sum = 0\n",
    "    model_diversity_pos_sum = 0\n",
    "    cos_in_topic_neg_sum = 0\n",
    "    cos_bet_topic_neg_sum = 0\n",
    "    model_diversity_neg_sum = 0\n",
    "    cos_in_topic_neu_sum = 0\n",
    "    cos_bet_topic_neu_sum = 0\n",
    "    model_diversity_neu_sum = 0\n",
    "        \n",
    "    for i in range(first_count, last_count + 1):\n",
    "\n",
    "        saved_model_folder = f\"/shared/share_tm-finance/Final/Stored_model/{year_num}_year_window/three_models/{modeltype}/pos\"\n",
    "        pos_lda_model, pos_tr_grouped_sum, pos_te_grouped_sum = \\\n",
    "                train_model(saved_model_folder, pos_df, pos_headlines, topic_num, pos_cluster_num, \n",
    "                            tar_year, year_num, sentiment_type, save_model, i)\n",
    "        mean_score, ave_sim, model_diversity = calculate_score(pos_lda_model, vectorizer, embedding_model)\n",
    "        cos_in_topic_pos_sum += mean_score\n",
    "        cos_bet_topic_pos_sum += ave_sim\n",
    "        model_diversity_pos_sum += model_diversity\n",
    "    \n",
    "        saved_model_folder = f\"/shared/share_tm-finance/Final/Stored_model/{year_num}_year_window/three_models/{modeltype}/neg\"\n",
    "        neg_lda_model, neg_tr_grouped_sum, neg_te_grouped_sum = \\\n",
    "                train_model(saved_model_folder, neg_df, neg_headlines, topic_num, neg_cluster_num, \n",
    "                            tar_year, year_num, sentiment_type, save_model, i)\n",
    "        mean_score, ave_sim, model_diversity = calculate_score(neg_lda_model, vectorizer, embedding_model)\n",
    "        cos_in_topic_neg_sum += mean_score\n",
    "        cos_bet_topic_neg_sum += ave_sim\n",
    "        model_diversity_neg_sum += model_diversity\n",
    "        \n",
    "        saved_model_folder = f\"/shared/share_tm-finance/Final/Stored_model/{year_num}_year_window/three_models/{modeltype}/neu\"\n",
    "        neu_lda_model, neu_tr_grouped_sum, neu_te_grouped_sum = \\\n",
    "                train_model(saved_model_folder, neu_df, neu_headlines, topic_num, neu_cluster_num, \n",
    "                            tar_year, year_num, sentiment_type, save_model, i, True)\n",
    "        mean_score, ave_sim, model_diversity = calculate_score(neu_lda_model, vectorizer, embedding_model)\n",
    "        cos_in_topic_neu_sum += mean_score\n",
    "        cos_bet_topic_neu_sum += ave_sim\n",
    "        model_diversity_neu_sum += model_diversity           \n",
    "\n",
    "        if combine:\n",
    "            #get the last column name of the last column of pos_tr_grouped_sum\n",
    "            pos_last_col = int(pos_tr_grouped_sum.columns[-1])\n",
    "            neg_tr_grouped_sum.columns = [str(int(col) + pos_last_col) for col in neg_tr_grouped_sum.columns]\n",
    "            neg_last_col = int(neg_tr_grouped_sum.columns[-1])\n",
    "            neu_tr_grouped_sum.columns = [str(int(col) + neg_last_col) for col in neu_tr_grouped_sum.columns]\n",
    "            \n",
    "            pos_last_col = int(pos_te_grouped_sum.columns[-1])\n",
    "            neg_te_grouped_sum.columns = [str(int(col) + pos_last_col) for col in neg_te_grouped_sum.columns]\n",
    "            neg_last_col = int(neg_te_grouped_sum.columns[-1])\n",
    "            neu_te_grouped_sum.columns = [str(int(col) + neg_last_col) for col in neu_te_grouped_sum.columns]\n",
    "\n",
    "            tr_grouped_sum = pd.concat([pos_tr_grouped_sum, neg_tr_grouped_sum, neu_tr_grouped_sum], axis = 1)\n",
    "            tr_grouped_sum.fillna(0, inplace = True)\n",
    "            te_grouped_sum = pd.concat([pos_te_grouped_sum, neg_te_grouped_sum, neu_te_grouped_sum], axis = 1)\n",
    "            te_grouped_sum.fillna(0, inplace = True)\n",
    "            linear_regression(tr_grouped_sum, te_grouped_sum, insampler2_list, outsampler2_list)\n",
    "        else:\n",
    "            linear_regression(pos_tr_grouped_sum, pos_te_grouped_sum, pos_insampler2_list, pos_outsampler2_list)\n",
    "            linear_regression(neg_tr_grouped_sum, neg_te_grouped_sum, neg_insampler2_list, neg_outsampler2_list)\n",
    "            linear_regression(neu_tr_grouped_sum, neu_te_grouped_sum, neu_insampler2_list, neu_outsampler2_list)\n",
    "    \n",
    "    if combine:\n",
    "        insampler2_dict[tar_year] = insampler2_list.mean()\n",
    "        outsampler2_dict[tar_year] = outsampler2_list.mean()\n",
    "    else:\n",
    "        sep_insampler2_list = (pos_insampler2_list* len(pos_embeddings) / len(embeddings)) + (neg_insampler2_list* len(neg_embeddings) / len(embeddings)) + (neu_insampler2_list* len(neu_embeddings) / len(embeddings))\n",
    "        sep_outsampler2_list = (pos_outsampler2_list* len(pos_embeddings) / len(embeddings)) + (neg_outsampler2_list* len(neg_embeddings) / len(embeddings)) + (neu_outsampler2_list* len(neu_embeddings) / len(embeddings))\n",
    "        insampler2_dict[tar_year] = sep_insampler2_list.mean()\n",
    "        insampler2_dict[tar_year] = sep_outsampler2_list.mean()\n",
    "\n",
    "    cos_in_topic_dict[tar_year] = (cos_in_topic_pos_sum* len(pos_embeddings) / len(embeddings) + cos_in_topic_neg_sum* len(neg_embeddings) / len(embeddings) + cos_in_topic_neu_sum* len(neu_embeddings) / len(embeddings)) / div\n",
    "    cos_bet_topic_dict[tar_year] = (cos_bet_topic_pos_sum* len(pos_embeddings) / len(embeddings) + cos_bet_topic_neg_sum* len(neg_embeddings) / len(embeddings) + cos_bet_topic_neu_sum* len(neu_embeddings) / len(embeddings)) / div\n",
    "    model_diversity_dict[tar_year] = (model_diversity_pos_sum* len(pos_embeddings) / len(embeddings) + model_diversity_neg_sum* len(neg_embeddings) / len(embeddings) + model_diversity_neu_sum* len(neu_embeddings) / len(embeddings)) / div\n",
    "\n",
    "    print(\"Year {year} is done\".format(year = tar_year))\n",
    "\n",
    "if not combine:\n",
    "    print(\"sep_insample = \", insampler2_dict)\n",
    "    print(\"sep_outsample = \", insampler2_dict)\n",
    "else:\n",
    "    print(\"insample = \", insampler2_dict)\n",
    "    print(\"outsample = \", outsampler2_dict)\n",
    "\n",
    "print(\"cos_in_topic = \", cos_in_topic_dict)\n",
    "print(\"cos_bet_topic = \", cos_bet_topic_dict)\n",
    "print(\"model_diversity = \", model_diversity_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{topic_num}_Data.xlsx'\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    "program_list = [insampler2_dict, outsampler2_dict, cos_in_topic_dict, cos_bet_topic_dict, model_diversity_dict]\n",
    "data_list = ['Insample_R2', 'Outsample_R2', 'Cos_in_topic', 'Cos_btn_topic', 'Diversity']\n",
    "\n",
    "for i in range(len(data_list)):\n",
    "    ws = wb[data_list[i]]\n",
    "    for cell in ws['A']:\n",
    "        if cell.value == models:\n",
    "            for row_num in range(cell.row, cell.row + 17):\n",
    "                sentiment_type_cell = ws.cell(row=row_num, column=2)\n",
    "                if sentiment_type_cell.value == sentiment_type:\n",
    "                    for row_num in range(sentiment_type_cell.row, sentiment_type_cell.row + 4):\n",
    "                        model_cell = ws.cell(row=row_num, column=3).value\n",
    "                        if model_cell == f\"{modeltype}_{topic_num}\":\n",
    "                            for year, value in program_list[i].items():\n",
    "                                ws.cell(row=row_num, column=year - 2013 + 3, value=value)\n",
    "                    break\n",
    "wb.save(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
