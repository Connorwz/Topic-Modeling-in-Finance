{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/rl3444/.conda/envs/NLP_Env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import openpyxl\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from lda import LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# import torch\n",
    "# import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "# Check available GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardization(css_sum_by_topic):\n",
    "    css_sum_by_topic_df = css_sum_by_topic.reset_index()\n",
    "    css_sum_by_topic_df.columns = ['topic', 'css']\n",
    "    scaler = StandardScaler()\n",
    "    css_sum_by_topic_df['css_standardized'] = scaler.fit_transform(css_sum_by_topic_df[['css']])\n",
    "    css_standardized_series = css_sum_by_topic_df.set_index('topic')['css_standardized']\n",
    "    return css_standardized_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(tr_grouped_sum, te_grouped_sum, insampler2_list, outsampler2_list):\n",
    "    tr_X = np.array(tr_grouped_sum)\n",
    "    tr_ret = [ind[2] for ind in list(tr_grouped_sum.index)]\n",
    "    tr_Y = np.array(tr_ret).reshape(-1,1)\n",
    "    tr_regression = LinearRegression(fit_intercept=True)\n",
    "    tr_regression.fit(tr_X,tr_Y)\n",
    "    tr_Y_pred = tr_regression.predict(tr_X)\n",
    "    tr_Y_mean = np.mean(tr_Y)\n",
    "    tr_SS_tot = np.sum((tr_Y - tr_Y_mean) ** 2)\n",
    "    tr_SS_res = np.sum((tr_Y - tr_Y_pred) ** 2)\n",
    "    tr_r2 = 1 - (tr_SS_res / tr_SS_tot)\n",
    "    insampler2_list.append(tr_r2)\n",
    "\n",
    "    te_X = np.array(te_grouped_sum)\n",
    "    te_ret = [ind[2] for ind in list(te_grouped_sum.index)]\n",
    "    te_Y = np.array(te_ret).reshape(-1,1)\n",
    "    te_Y_pred = tr_regression.predict(te_X)\n",
    "    te_SS_tot = np.sum((te_Y - tr_Y_mean) ** 2)\n",
    "    te_SS_res = np.sum((te_Y - te_Y_pred) ** 2)\n",
    "    te_r2 = 1 - (te_SS_res / te_SS_tot)\n",
    "    outsampler2_list.append(te_r2)\n",
    "\n",
    "    return\n",
    "\n",
    "def train_model(saved_model_folder, df, red_headlines, modeltype, topic_num, cluster_num, tar_year, sentiment_type, save_model, i, neutral = False):\n",
    "    # Perform the train-test split on indices\n",
    "    indices = np.arange(len(red_headlines))\n",
    "    tr_ind, te_ind = train_test_split(indices, test_size=0.2, shuffle= True, random_state=i)\n",
    "\n",
    "    # torch.cuda.empty_cache()\n",
    "    # gc.collect()\n",
    "    tr_df = df.iloc[tr_ind,:]\n",
    "    te_df = df.iloc[te_ind,:]\n",
    "    tr_headlines = [red_headlines[ind] for ind in tr_ind]\n",
    "    te_headlines = [red_headlines[ind] for ind in te_ind]\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    tr_doc_term = vectorizer.fit_transform(tr_headlines)\n",
    "    te_doc_term = vectorizer.fit_transform(te_headlines)\n",
    "\n",
    "    lda_model = LDA(n_topics = cluster_num, n_iter = 100, random_state = 66)\n",
    "    lda_model.fit(tr_doc_term)\n",
    "\n",
    "    #save the topic model\n",
    "    if save_model == True:\n",
    "        os.makedirs(saved_model_folder+f'/{sentiment_type}/{modeltype}/{tar_year}_{topic_num}_{i}', exist_ok = True)            \n",
    "        with open(saved_model_folder+f'/{sentiment_type}/{modeltype}/{tar_year}_{topic_num}_{i}/{tar_year}_{topic_num}_{i}_model', 'wb') as file:\n",
    "            pickle.dump(lda_model,file)\n",
    "    \n",
    "    #calculate insample R2\n",
    "    tr_topic_dist = lda_model.doc_topic_\n",
    "    # tr_df = tr_df.reset_index(drop = True)\n",
    "    tr_contem_ret_topic_dist = pd.concat([tr_df.drop(columns = [\"rp_entity_id\",\"headline\",\"vocab_con_headline\"]),pd.DataFrame(tr_topic_dist)],axis = 1)\n",
    "    tr_grouped = tr_contem_ret_topic_dist.groupby(['date',\"comnam\",\"ret\"])\n",
    "    tr_grouped_sum = tr_grouped.sum()\n",
    "    data = []\n",
    "    for i, topic_dist in enumerate(tr_topic_dist):\n",
    "        topic = topic_dist.argmax()\n",
    "        data.append({'Document': i, 'Topic': topic, 'Topic Distribution': topic_dist})\n",
    "    Topic_df = pd.DataFrame(data)\n",
    "    topics = Topic_df[\"Topic\"].to_numpy()\n",
    "    \n",
    "    if sentiment_type == \"per_topic\":\n",
    "        tr_df['topic'] = topics\n",
    "        tr_css_sum_by_topic = tr_df.groupby('topic')['css'].sum()\n",
    "        tr_df.drop(columns = ['topic'], inplace = True)\n",
    "        tr_css_standardized_series = Standardization(tr_css_sum_by_topic)\n",
    "        tr_grouped_sum.iloc[:, 1:] = tr_grouped_sum.iloc[:, 1:].mul(tr_css_standardized_series, axis=1)\n",
    "    elif sentiment_type == \"per_ret\":\n",
    "        if not neutral:\n",
    "            tr_grouped_sum.iloc[:, 1:] = tr_grouped_sum.iloc[:, 1:].mul(tr_grouped_sum['css'], axis=0)\n",
    "        tr_grouped_sum.drop(columns = ['css'], inplace = True)\n",
    "    elif sentiment_type == \"only_senti\":\n",
    "        tr_grouped_sum = tr_grouped_sum[['css']]\n",
    "    elif sentiment_type == \"no_senti\":\n",
    "        tr_grouped_sum.drop(columns = ['css'], inplace = True)   \n",
    "\n",
    "    #calculate outsample R2\n",
    "    te_topic_dist = lda_model.transform(te_doc_term)\n",
    "    #te_df = te_df.reset_index(drop = True)\n",
    "    te_contem_ret_topic_dist = pd.concat([te_df.drop(columns = [\"rp_entity_id\",\"headline\",\"vocab_con_headline\"]),pd.DataFrame(te_topic_dist)],axis = 1)\n",
    "    te_grouped = te_contem_ret_topic_dist.groupby(['date',\"comnam\",\"ret\"])\n",
    "    te_grouped_sum = te_grouped.sum()\n",
    "    new_data = []\n",
    "    for i, topic_dist in enumerate(te_topic_dist):\n",
    "        topic = topic_dist.argmax()\n",
    "        new_data.append({'Document': i, 'Topic': topic, 'Topic Distribution': topic_dist})\n",
    "    new_Topic_df = pd.DataFrame(new_data)\n",
    "    new_topics = new_Topic_df[\"Topic\"].to_numpy()\n",
    "    \n",
    "    if sentiment_type == \"per_topic\":\n",
    "        te_df['topic'] = new_topics\n",
    "        te_css_sum_by_topic = te_df.groupby('topic')['css'].sum()\n",
    "        te_df.drop(columns = ['topic'], inplace = True)\n",
    "        te_css_standardized_series = Standardization(te_css_sum_by_topic)\n",
    "        te_grouped_sum.iloc[:, 1:] = te_grouped_sum.iloc[:, 1:].mul(te_css_standardized_series, axis=1)\n",
    "    elif sentiment_type == \"per_ret\":\n",
    "        if not neutral:\n",
    "            te_grouped_sum.iloc[:, 1:] = te_grouped_sum.iloc[:, 1:].mul(te_grouped_sum['css'], axis=0)\n",
    "        te_grouped_sum.drop(columns = ['css'], inplace = True)\n",
    "    elif sentiment_type == \"only_senti\":\n",
    "        te_grouped_sum = te_grouped_sum[['css']]\n",
    "    elif sentiment_type == \"no_senti\":\n",
    "        te_grouped_sum.drop(columns = ['css'], inplace = True)\n",
    "\n",
    "    return lda_model, tr_topic_dist, tr_grouped_sum, te_grouped_sum, Topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate pairwise cosine distances\n",
    "def calculate_pairwise_cosine_distances(embeddings):\n",
    "    return cosine_distances(embeddings)\n",
    "\n",
    "# Calculate Model Diversity Score\n",
    "def compute_model_diversity(topics):\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "\n",
    "    for topic in topics:\n",
    "        total_words += len(topic)\n",
    "        for words in topic:\n",
    "            unique_words.add(words)\n",
    "    diversity_score = len(unique_words) / total_words\n",
    "    return diversity_score\n",
    "\n",
    "def load_model(saved_model_folder, modeltype, tar_year, cluster_num, count_num, sentiment_type):\n",
    "    # indices = np.arange(len(red_headlines))\n",
    "    # tr_ind, te_ind = train_test_split(indices, test_size=0.2, shuffle= True, random_state=count_num)\n",
    "    # tr_df = df.iloc[tr_ind,:]\n",
    "    # te_df = df.iloc[te_ind,:]\n",
    "    # tr_headlines = [red_headlines[ind] for ind in tr_ind]\n",
    "    # te_headlines = [red_headlines[ind] for ind in te_ind]\n",
    "    # tr_embeddings = embeddings[tr_ind,:]\n",
    "    # te_embeddings = embeddings[te_ind,:]\n",
    "\n",
    "    model_path = f'{saved_model_folder}/{sentiment_type}/{modeltype}/{tar_year}_{cluster_num}_{count_num}/{tar_year}_{cluster_num}_{count_num}_model'\n",
    "    with open(model_path, 'rb') as file:\n",
    "        lda_model = pickle.load(file)\n",
    "    topic_dist = lda_model.doc_topic_\n",
    "    return lda_model, topic_dist\n",
    "\n",
    "def calculate_score(lda_model, topic_dist, vectorizer, embedding_model):\n",
    "    data = []\n",
    "    for i, topic_dist_data in enumerate(topic_dist):\n",
    "        topic = topic_dist_data.argmax()\n",
    "        data.append({'Document': i, 'Topic': topic, 'Topic Distribution': topic_dist_data})\n",
    "    Topic_df = pd.DataFrame(data)\n",
    "    topics = Topic_df[\"Topic\"].to_numpy()\n",
    "\n",
    "    topic_words = {}\n",
    "    topic_words_list = []\n",
    "    topic_info = {}\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "        \n",
    "    for topic_idx, topic_dist_data in enumerate(lda_model.topic_word_):\n",
    "        top_words_idx = np.argsort(topic_dist_data)[::-1][:10]  # Get top 10 words for this topic\n",
    "        top_words = [vocab[i] for i in top_words_idx]\n",
    "        topic_words[topic_idx] = top_words\n",
    "        top_words_freq = [(vocab[i], topic_dist_data[i]) for i in top_words_idx]\n",
    "        topic_info[topic_idx] = top_words_freq\n",
    "        topic_words_list.append(top_words)\n",
    "\n",
    "        # Get embeddings for the words in each topic\n",
    "    topic_embeddings = {}\n",
    "    for topic, words in topic_words.items():\n",
    "        embeddings = embedding_model.encode(words)\n",
    "        topic_embeddings[topic] = embeddings\n",
    "\n",
    "    #calculate in-topic similarity score\n",
    "    topic_distances = {}\n",
    "    for topic, embeddings in topic_embeddings.items():\n",
    "        distances = calculate_pairwise_cosine_distances(embeddings)\n",
    "        topic_distances[topic] = distances\n",
    "\n",
    "    # Aggregate the scores by taking the mean distance\n",
    "    topic_scores = {}\n",
    "    for topic, distances in topic_distances.items():\n",
    "        mean_distance = np.mean(distances)\n",
    "        topic_scores[topic] = mean_distance\n",
    "\n",
    "    # # Display the scores for each topic\n",
    "    # for topic, score in topic_scores.items():\n",
    "    #     print(f\"Topic {topic}: Mean Cosine Distance = {score}\")\n",
    "\n",
    "    #calculate the mean score\n",
    "    mean_score = np.mean(list(topic_scores.values()))\n",
    "\n",
    "    #calculate between-topic similarity score\n",
    "    sim = 0\n",
    "    count = 0\n",
    "    for topic1, topic2 in combinations(topic_info, 2):\n",
    "        centroid1 = np.mean(topic_embeddings[topic1], axis=0)\n",
    "        centroid2 = np.mean(topic_embeddings[topic2], axis=0)\n",
    "        sim += 1 - cosine(centroid1, centroid2)\n",
    "        count += 1\n",
    "\n",
    "    model_diversity = compute_model_diversity(topic_words_list)\n",
    "    return mean_score, sim / count, model_diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insampler2_dict = {}\n",
    "outsampler2_dict = {}\n",
    "cos_in_topic_dict = {}\n",
    "cos_bet_topic_dict = {}\n",
    "model_diversity_dict = {}\n",
    "cos_in_topic_dic = {}\n",
    "cos_bet_topic_dic = {}\n",
    "model_diversity_dic = {}\n",
    "\n",
    "modeltype = 'lda'\n",
    "sentiment_type = 'per_ret'\n",
    "topic_num = 120\n",
    "save_model = False\n",
    "sentiment_sign = False\n",
    "combine = False\n",
    "datatype = 'contem'\n",
    "df_folder = \"/shared/share_tm-finance/Processed_df_Sentiment/One_year_window\"\n",
    "embeddings_folder = \"/shared/share_tm-finance/Embeddings_with_Sentiment/One_year_window\"\n",
    "saved_model_folder = \"/shared/share_tm-finance/Stored_model/new_data\"\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "first_count = 1\n",
    "last_count = 5\n",
    "div = last_count - first_count + 1\n",
    "\n",
    "for i in range(2023, 2024):\n",
    "    \n",
    "    tar_year = i\n",
    "    df = pd.read_csv(df_folder+'/{type}_{year}_senti.csv'.format(year = tar_year, type = datatype))\n",
    "    red_headlines = df.vocab_con_headline.tolist()\n",
    "    embeddings = np.load(embeddings_folder+\"/{type}_{year}_senti_embeddings.npy\".format(year = tar_year, type = datatype))\n",
    "    indices = np.arange(len(red_headlines))\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(red_headlines)\n",
    "\n",
    "    pos_insampler2_list = []\n",
    "    pos_outsampler2_list = []\n",
    "    neg_insampler2_list = []\n",
    "    neg_outsampler2_list = []\n",
    "    neu_insampler2_list = []\n",
    "    neu_outsampler2_list = []\n",
    "    insampler2_list = []\n",
    "    outsampler2_list = []\n",
    "    \n",
    "    pos_indices = df[df['css'] > 0].index\n",
    "    neg_indices = df[df['css'] < 0].index\n",
    "    neu_indices = df[df['css'] == 0].index\n",
    "    pos_df = df.iloc[pos_indices,:]\n",
    "    neg_df = df.iloc[neg_indices,:]\n",
    "    neu_df = df.iloc[neu_indices,:]\n",
    "    pos_headlines = [red_headlines[ind] for ind in pos_indices]\n",
    "    neg_headlines = [red_headlines[ind] for ind in neg_indices]\n",
    "    neu_headlines = [red_headlines[ind] for ind in neu_indices]\n",
    "    pos_embeddings = embeddings[pos_indices,:]\n",
    "    neg_embeddings = embeddings[neg_indices,:]\n",
    "    neu_embeddings = embeddings[neu_indices,:]\n",
    "    \n",
    "    cos_in_topic_pos_sum = 0\n",
    "    cos_bet_topic_pos_sum = 0\n",
    "    model_diversity_pos_sum = 0\n",
    "    cos_in_topic_neg_sum = 0\n",
    "    cos_bet_topic_neg_sum = 0\n",
    "    model_diversity_neg_sum = 0\n",
    "    cos_in_topic_neu_sum = 0\n",
    "    cos_bet_topic_neu_sum = 0\n",
    "    model_diversity_neu_sum = 0\n",
    "\n",
    "    #set pos_cluster_num, neg_cluster_num, neu_cluster_num based on the number of embeddings\n",
    "    pos_cluster_num = int(topic_num * len(pos_embeddings) / len(embeddings))\n",
    "    neg_cluster_num = int(topic_num * len(neg_embeddings) / len(embeddings))\n",
    "    neu_cluster_num = int(topic_num * len(neu_embeddings) / len(embeddings))\n",
    "    diff = topic_num - (pos_cluster_num + neg_cluster_num + neu_cluster_num)\n",
    "    if pos_cluster_num < neg_cluster_num and pos_cluster_num < neu_cluster_num:\n",
    "        pos_cluster_num += diff\n",
    "    elif neg_cluster_num < pos_cluster_num and neg_cluster_num < neu_cluster_num:\n",
    "        neg_cluster_num += diff\n",
    "    else:\n",
    "        neu_cluster_num += diff\n",
    "    \n",
    "    cos_in_topic_pos_sum = 0\n",
    "    cos_bet_topic_pos_sum = 0\n",
    "    model_diversity_pos_sum = 0\n",
    "    cos_in_topic_neg_sum = 0\n",
    "    cos_bet_topic_neg_sum = 0\n",
    "    model_diversity_neg_sum = 0\n",
    "    cos_in_topic_neu_sum = 0\n",
    "    cos_bet_topic_neu_sum = 0\n",
    "    model_diversity_neu_sum = 0\n",
    "        \n",
    "    for i in range(first_count, last_count + 1):\n",
    "\n",
    "        saved_model_folder = \"/shared/share_tm-finance/Stored_model/three_models/pos_topic\"\n",
    "        pos_lda_model, pos_tr_topic_dist, pos_tr_grouped_sum, pos_te_grouped_sum, pos_Topic_df = \\\n",
    "                train_model(saved_model_folder, pos_df, pos_headlines, modeltype, topic_num, pos_cluster_num, tar_year, sentiment_type, save_model, i)\n",
    "        mean_score, ave_sim, model_diversity = calculate_score(pos_lda_model, pos_tr_topic_dist, vectorizer, embedding_model)\n",
    "        cos_in_topic_pos_sum += mean_score\n",
    "        cos_bet_topic_pos_sum += ave_sim\n",
    "        model_diversity_pos_sum += model_diversity\n",
    "       \n",
    "        saved_model_folder = \"/shared/share_tm-finance/Stored_model/three_models/neg_topic\"\n",
    "        neg_lda_model, neg_tr_topic_dist, neg_tr_grouped_sum, neg_te_grouped_sum, neg_Topic_df = \\\n",
    "                train_model(saved_model_folder, neg_df, neg_headlines, modeltype, topic_num, neg_cluster_num, tar_year, sentiment_type, save_model, i)\n",
    "        mean_score, ave_sim, model_diversity = calculate_score(neg_lda_model, neg_tr_topic_dist, vectorizer, embedding_model)\n",
    "        cos_in_topic_neg_sum += mean_score\n",
    "        cos_bet_topic_neg_sum += ave_sim\n",
    "        model_diversity_neg_sum += model_diversity\n",
    "        \n",
    "        saved_model_folder = \"/shared/share_tm-finance/Stored_model/three_models/neu_topic\"\n",
    "        neu_lda_model, neu_tr_topic_dist, neu_tr_grouped_sum, neu_te_grouped_sum, neu_Topic_df = \\\n",
    "                train_model(saved_model_folder, neu_df, neu_headlines, modeltype, topic_num, neu_cluster_num, tar_year, sentiment_type, save_model, i, True)\n",
    "        mean_score, ave_sim, model_diversity = calculate_score(neu_lda_model, neu_tr_topic_dist, vectorizer, embedding_model)\n",
    "        cos_in_topic_neu_sum += mean_score\n",
    "        cos_bet_topic_neu_sum += ave_sim\n",
    "        model_diversity_neu_sum += model_diversity        \n",
    "\n",
    "        if combine:\n",
    "            #get the last column name of the last column of pos_tr_grouped_sum\n",
    "            pos_last_col = int(pos_tr_grouped_sum.columns[-1])\n",
    "            neg_tr_grouped_sum.columns = [str(int(col) + pos_last_col) for col in neg_tr_grouped_sum.columns]\n",
    "            neg_last_col = int(neg_tr_grouped_sum.columns[-1])\n",
    "            neu_tr_grouped_sum.columns = [str(int(col) + neg_last_col) for col in neu_tr_grouped_sum.columns]\n",
    "            \n",
    "            pos_last_col = int(pos_te_grouped_sum.columns[-1])\n",
    "            neg_te_grouped_sum.columns = [str(int(col) + pos_last_col) for col in neg_te_grouped_sum.columns]\n",
    "            neg_last_col = int(neg_te_grouped_sum.columns[-1])\n",
    "            neu_te_grouped_sum.columns = [str(int(col) + neg_last_col) for col in neu_te_grouped_sum.columns]\n",
    "\n",
    "            tr_grouped_sum = pd.concat([pos_tr_grouped_sum, neg_tr_grouped_sum, neu_tr_grouped_sum], axis = 1)\n",
    "            tr_grouped_sum.fillna(0, inplace = True)\n",
    "            te_grouped_sum = pd.concat([pos_te_grouped_sum, neg_te_grouped_sum, neu_te_grouped_sum], axis = 1)\n",
    "            te_grouped_sum.fillna(0, inplace = True)\n",
    "            linear_regression(tr_grouped_sum, te_grouped_sum, insampler2_list, outsampler2_list)\n",
    "        else:\n",
    "            linear_regression(pos_tr_grouped_sum, pos_te_grouped_sum, pos_insampler2_list, pos_outsampler2_list)\n",
    "            linear_regression(neg_tr_grouped_sum, neg_te_grouped_sum, neg_insampler2_list, neg_outsampler2_list)\n",
    "            linear_regression(neu_tr_grouped_sum, neu_te_grouped_sum, neu_insampler2_list, neu_outsampler2_list)\n",
    "    \n",
    "    if combine:\n",
    "        insampler2_dict[tar_year] = insampler2_list.mean()\n",
    "        outsampler2_dict[tar_year] = outsampler2_list.mean()\n",
    "    else:\n",
    "        sep_insampler2_list = (pos_insampler2_list* len(pos_embeddings) / len(embeddings)) + (neg_insampler2_list* len(neg_embeddings) / len(embeddings)) + (neu_insampler2_list* len(neu_embeddings) / len(embeddings))\n",
    "        sep_outsampler2_list = (pos_outsampler2_list* len(pos_embeddings) / len(embeddings)) + (neg_outsampler2_list* len(neg_embeddings) / len(embeddings)) + (neu_outsampler2_list* len(neu_embeddings) / len(embeddings))\n",
    "        insampler2_dict[tar_year] = sep_insampler2_list.mean()\n",
    "        insampler2_dict[tar_year] = sep_outsampler2_list.mean()\n",
    "\n",
    "    cos_in_topic_dict[tar_year] = (cos_in_topic_pos_sum* len(pos_embeddings) / len(embeddings) + cos_in_topic_neg_sum* len(neg_embeddings) / len(embeddings) + cos_in_topic_neu_sum* len(neu_embeddings) / len(embeddings)) / div\n",
    "    cos_bet_topic_dict[tar_year] = (cos_bet_topic_pos_sum* len(pos_embeddings) / len(embeddings) + cos_bet_topic_neg_sum* len(neg_embeddings) / len(embeddings) + cos_bet_topic_neu_sum* len(neu_embeddings) / len(embeddings)) / div\n",
    "    model_diversity_dict[tar_year] = (model_diversity_pos_sum* len(pos_embeddings) / len(embeddings) + model_diversity_neg_sum* len(neg_embeddings) / len(embeddings) + model_diversity_neu_sum* len(neu_embeddings) / len(embeddings)) / div\n",
    "\n",
    "    print(\"Year {year} is done\".format(year = tar_year))\n",
    "\n",
    "if not combine:\n",
    "    print(\"sep_insample = \", insampler2_dict)\n",
    "    print(\"sep_outsample = \", insampler2_dict)\n",
    "else:\n",
    "    print(\"insample = \", insampler2_dict)\n",
    "    print(\"outsample = \", outsampler2_dict)\n",
    "\n",
    "print(\"cos_in_topic = \", cos_in_topic_dict)\n",
    "print(\"cos_bet_topic = \", cos_bet_topic_dict)\n",
    "print(\"model_diversity = \", model_diversity_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Data.xlsx'\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    "program_list = [insampler2_dict, outsampler2_dict, cos_in_topic_dict, cos_bet_topic_dict, model_diversity_dict]\n",
    "data_list = ['Insample_R2', 'Outsample_R2', 'Cos_in_topic', 'Cos_btn_topic', 'Diversity']\n",
    "\n",
    "for i in len(data_list):\n",
    "    ws = wb[data_list[i]]\n",
    "    for cell in ws['A']:\n",
    "        if cell.value != None:\n",
    "            for row_num in range(cell.row, cell.row + 3):\n",
    "                model_cell = ws.cell(row=row_num, column=2).value\n",
    "                if model_cell == f\"{modeltype}_{topic_num}\":\n",
    "                    for year, value in program_list[i].items():\n",
    "                        ws.cell(row=row_num, column=year - 2013 + 2, value=value)\n",
    "wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Data.xlsx'\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    "data_list = ['Insample_R2', 'Outsample_R2', 'Cos_in_topic', 'Cos_btn_topic', 'Diversity']\n",
    "years = range(2014, 2024)\n",
    "for data_type in data_list:\n",
    "    ws = wb[data_type]\n",
    "    for cell in ws['A']:\n",
    "        if cell.value != None:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.title(f'{cell.value} {data_type}')\n",
    "            for row_num in range(cell.row, cell.row + 3):\n",
    "                model_cell = ws.cell(row=row_num, column=2).value\n",
    "                data = []\n",
    "                for year in years:\n",
    "                    data.append(ws.cell(row=row_num, column=year - 2013 + 2).value)\n",
    "                plt.plot(years, data, label = model_cell)\n",
    "            plt.xlabel('Year')\n",
    "            plt.ylabel(f'{data_type}')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'graph/{cell.value}_{data_type}.png')\n",
    "            plt.close()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
