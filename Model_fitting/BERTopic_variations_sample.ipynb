{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os \n",
    "import pandas as pd \n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Take PCA and KMeans as the example here\n",
    "from cuml.decomposition import PCA\n",
    "# from cuml.cluster import KMeans\n",
    "from pycave.bayes import GaussianMixture as GMM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_folder = \"/scratch/wx2309/Processed_data/one_year_window\"\n",
    "embeddings_folder = \"/scratch/wx2309/embeddings\"\n",
    "df = pd.read_csv(df_folder+\"/contem_2023.csv\")\n",
    "headlines = df.headline.tolist()\n",
    "embeddings = np.load(embeddings_folder+\"/contem_2023_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1749770/1749770 [00:07<00:00, 233618.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41353"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = collections.Counter()\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "for headline in tqdm(headlines):\n",
    "    vocab.update(tokenizer(headline))\n",
    "vocab = [word for word, count in vocab.items() if count > 20]\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGMM:\n",
    "    def __init__(self, num_components, trainer_params):\n",
    "        self.gmm = GMM(num_components=num_components,trainer_params=trainer_params)\n",
    "        self.labels_ = None\n",
    "    \n",
    "    def fit(self,data):\n",
    "        self.gmm.fit(data)\n",
    "        self.labels_ = np.array(self.gmm.predict(data))\n",
    "        return self\n",
    "    \n",
    "    def predict(self,data):\n",
    "        return np.array(self.gmm.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "umap_model = PCA(n_components = 10)\n",
    "# Assume 62 clusters here\n",
    "hdbscan_model = MyGMM(num_components=62,trainer_params={\"accelerator\":'gpu',\"devices\":1})\n",
    "vectorizer_model = CountVectorizer(vocabulary=vocab,stop_words=\"english\")\n",
    "Topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model,\n",
    "                    calculate_probabilities = False,verbose = True,low_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 23:51:41,431 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-06-17 23:51:44,071 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-06-17 23:51:44,109 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "Fitting K-means estimator...\n",
      "Running initialization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122: 100%|██████████| 1/1 [00:00<00:00, 106.76it/s]\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 109.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting K-Means...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 299: 100%|██████████| 1/1 [00:00<00:00, 41.05it/s, inertia=0.0527]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running initialization...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 57.32it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Gaussian mixture...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00, 54.03it/s, nll=-8.54] \n",
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  6.98it/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 23:51:57,233 - BERTopic - Cluster - Completed ✓\n",
      "2024-06-17 23:51:57,415 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "/user/wx2309/.conda/envs/TM/lib/python3.10/site-packages/bertopic/vectorizers/_ctfidf.py:82: RuntimeWarning: divide by zero encountered in divide\n",
      "  idf = np.log((avg_nr_samples / df)+1)\n",
      "2024-06-17 23:52:07,869 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x7fa728ce9810>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Topic_model.fit(headlines,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1750/1750 [01:45<00:00, 16.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error is 0.00038033350059003433\n",
      "Testing error is 0.0003901718257529071\n",
      "R square is 0.007925715186710591\n"
     ]
    }
   ],
   "source": [
    "topic_dist, _ = Topic_model.approximate_distribution(headlines)\n",
    "contem_ret_topic_dist = pd.concat([df.drop(columns = [\"rp_entity_id\",\"headline\"]),pd.DataFrame(topic_dist)],axis = 1)\n",
    "grouped = contem_ret_topic_dist.groupby(['date',\"comnam\",\"ret\"])\n",
    "grouped_sum = grouped.sum()\n",
    "\n",
    "X = np.array(grouped_sum)\n",
    "ret = [ind[2] for ind in list(grouped_sum.index)]\n",
    "Y = np.array(ret).reshape(-1,1)\n",
    "X_tr, X_te, Y_tr, Y_te = train_test_split(X,Y,test_size=0.2,random_state=66)\n",
    "regression = LinearRegression(fit_intercept=True)\n",
    "regression.fit(X_tr,Y_tr)\n",
    "Y_tr_pred = regression.predict(X_tr)\n",
    "Y_te_pred = regression.predict(X_te)\n",
    "mse_tr = mean_squared_error(Y_tr,Y_tr_pred)\n",
    "mse_te= mean_squared_error(Y_te,Y_te_pred)\n",
    "regression.fit(X,Y)\n",
    "R_square = regression.score(X,Y)\n",
    "\n",
    "print(f\"Training error is {mse_tr}\")\n",
    "print(f\"Testing error is {mse_te}\")\n",
    "print(f\"R square is {R_square}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_TM",
   "language": "python",
   "name": "tm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
