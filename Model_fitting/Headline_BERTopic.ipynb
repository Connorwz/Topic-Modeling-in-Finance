{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "Imporint packages, setting up GPU and functions for BERTopic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openpyxl\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.cluster import BaseCluster\n",
    "import re\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from cuml.manifold import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from cuml.cluster import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "# Check available GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X)\n",
    "        self.labels_ = self.model.predict(X)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = self.model.predict(X)\n",
    "        self.labels_ = predictions\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_setup(modeltype, cluster_num, hdb_min_cluster_size):    \n",
    "    # Embeddings\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    # Reduce dimensionality\n",
    "    PCA_model = PCA(n_components=10)\n",
    "    SVD_model = TruncatedSVD(n_components=10, random_state=42, n_iter=10) \n",
    "    umap_model = UMAP(n_neighbors=10, n_components=10, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    # Cluster embeddings\n",
    "    gmm_model = ClusterModel(GaussianMixture(n_components=cluster_num, covariance_type='full'))\n",
    "    KMeans_model = KMeans(n_clusters=cluster_num)\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=hdb_min_cluster_size, metric = \"euclidean\", cluster_selection_method=\"eom\",\n",
    "                            gen_min_span_tree = True, prediction_data = True, min_samples = 20, verbose = True)\n",
    "    # Vectorize\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=1, max_df = 50, ngram_range=(1, 2))\n",
    "    \n",
    "    model_combinations = {\n",
    "    'umaphdbscan': (umap_model, hdbscan_model),\n",
    "    'pcakmeans': (PCA_model, KMeans_model),\n",
    "    'umapgmm': (umap_model, gmm_model),\n",
    "    'pcagmm': (PCA_model, gmm_model),\n",
    "    'svdkeans': (SVD_model, KMeans_model),\n",
    "    'pcahdbscan': (PCA_model, hdbscan_model)\n",
    "    }\n",
    "\n",
    "    dim_red_model, cluster_model = model_combinations.get(modeltype, (PCA_model, KMeans_model))\n",
    "\n",
    "    Topic_model = BERTopic(embedding_model=embedding_model, umap_model=dim_red_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model,\n",
    "                        calculate_probabilities = False, verbose = True, low_memory = True)\n",
    "    \n",
    "    return Topic_model, embedding_model\n",
    "\n",
    "\n",
    "def Standardization(css_sum_by_topic):\n",
    "    css_sum_by_topic_df = css_sum_by_topic.reset_index()\n",
    "    css_sum_by_topic_df.columns = ['topic', 'css']\n",
    "    scaler = StandardScaler()\n",
    "    css_sum_by_topic_df['css_standardized'] = scaler.fit_transform(css_sum_by_topic_df[['css']])\n",
    "    css_standardized_series = css_sum_by_topic_df.set_index('topic')['css_standardized']\n",
    "    \n",
    "    return css_standardized_series\n",
    "    \n",
    "\n",
    "def train_model(saved_model_folder, df, red_headlines, embeddings, modeltype, topic_num, cluster_num, hdb_min_cluster_size, tar_year, year_num, sentiment_type, save_model, reduce_outliers, i, neutral = False):    \n",
    "    # Perform the train-test split on indices\n",
    "    indices = np.arange(len(red_headlines))\n",
    "    tr_ind, te_ind = train_test_split(indices, test_size=0.2, shuffle= True, random_state=i)\n",
    "\n",
    "    tr_df = df.iloc[tr_ind,:]\n",
    "    te_df = df.iloc[te_ind,:]\n",
    "    tr_headlines = [red_headlines[ind] for ind in tr_ind]\n",
    "    te_headlines = [red_headlines[ind] for ind in te_ind]\n",
    "    tr_embeddings = embeddings[tr_ind,:]\n",
    "    te_embeddings = embeddings[te_ind,:]\n",
    "    \n",
    "    Topic_model, embedding_model = model_setup(modeltype, cluster_num, hdb_min_cluster_size)\n",
    "    indices = np.arange(len(red_headlines))\n",
    "    \n",
    "    topics, probs = Topic_model.fit_transform(tr_headlines, embeddings = tr_embeddings)\n",
    "\n",
    "    #save the topic model\n",
    "    if save_model:\n",
    "        Topic_model.save(saved_model_folder+f'/{tar_year - year_num + 1}_{tar_year}_{topic_num}_{i}',\n",
    "        serialization = \"safetensors\", save_ctfidf = True, save_embedding_model = embedding_model)\n",
    "\n",
    "    #reduce outliers\n",
    "    if reduce_outliers:\n",
    "        topics = Topic_model.reduce_outliers(tr_headlines, topics)\n",
    "        Topic_model.update_topics(tr_headlines, topics=topics)\n",
    "    \n",
    "    #calculate insample R2\n",
    "    tr_topic_dist, _ = Topic_model.approximate_distribution(tr_headlines)\n",
    "    tr_df = tr_df.reset_index(drop = True)\n",
    "    tr_contem_ret_topic_dist = pd.concat([tr_df.drop(columns = [\"rp_entity_id\",\"headline\",\"vocab_con_headline\"]),pd.DataFrame(tr_topic_dist)],axis = 1)\n",
    "    tr_grouped = tr_contem_ret_topic_dist.groupby(['date',\"comnam\",\"ret\"])\n",
    "    tr_grouped_sum = tr_grouped.sum()\n",
    "    if sentiment_type == \"per_topic\":\n",
    "        tr_df['topic'] = topics\n",
    "        tr_css_sum_by_topic = tr_df.groupby('topic')['css'].sum()\n",
    "        tr_df.drop(columns = ['topic'], inplace = True)\n",
    "        tr_css_standardized_series = Standardization(tr_css_sum_by_topic)\n",
    "        if modeltype == 'pcahdbscan':\n",
    "            tr_css_standardized_series = tr_css_standardized_series[1:]\n",
    "        tr_grouped_sum.iloc[:, 1:] = tr_grouped_sum.iloc[:, 1:].mul(tr_css_standardized_series, axis=1)\n",
    "        tr_grouped_sum.drop(columns = ['css'], inplace = True)\n",
    "    elif sentiment_type == \"per_ret\":\n",
    "        if not neutral:\n",
    "            tr_grouped_sum.iloc[:, 1:] = tr_grouped_sum.iloc[:, 1:].mul(tr_grouped_sum['css'], axis=0)\n",
    "        tr_grouped_sum.drop(columns = ['css'], inplace = True)\n",
    "    elif sentiment_type == \"only_senti\":\n",
    "        tr_grouped_sum = tr_grouped_sum[['css']]\n",
    "    elif sentiment_type == \"no_senti\":\n",
    "        tr_grouped_sum.drop(columns = ['css'], inplace = True)  \n",
    "\n",
    "    \n",
    "    #calculate outsample R2\n",
    "    new_topics, new_probs = Topic_model.transform(te_headlines, embeddings = te_embeddings)\n",
    "\n",
    "    if reduce_outliers:\n",
    "        new_topics = Topic_model.reduce_outliers(te_headlines, new_topics)\n",
    "        Topic_model.update_topics(te_headlines, topics=new_topics)\n",
    "    \n",
    "    te_topic_dist, _ = Topic_model.approximate_distribution(te_headlines)     \n",
    "    te_df = te_df.reset_index(drop = True)\n",
    "    te_contem_ret_topic_dist = pd.concat([te_df.drop(columns = [\"rp_entity_id\",\"headline\",\"vocab_con_headline\"]),pd.DataFrame(te_topic_dist)],axis = 1)\n",
    "    te_grouped = te_contem_ret_topic_dist.groupby(['date',\"comnam\",\"ret\"])\n",
    "    te_grouped_sum = te_grouped.sum()\n",
    "    if sentiment_type == \"per_topic\":\n",
    "        te_df['topic'] = new_topics\n",
    "        te_css_sum_by_topic = te_df.groupby('topic')['css'].sum()\n",
    "        te_df.drop(columns = ['topic'], inplace = True)\n",
    "        te_css_standardized_series = Standardization(te_css_sum_by_topic)\n",
    "        if modeltype == 'pcahdbscan':\n",
    "            te_css_standardized_series = te_css_standardized_series[1:]\n",
    "        te_grouped_sum.iloc[:, 1:] = te_grouped_sum.iloc[:, 1:].mul(te_css_standardized_series, axis=1)\n",
    "        te_grouped_sum.drop(columns = ['css'], inplace = True)\n",
    "    elif sentiment_type == \"per_ret\":\n",
    "        if not neutral:\n",
    "            te_grouped_sum.iloc[:, 1:] = te_grouped_sum.iloc[:, 1:].mul(te_grouped_sum['css'], axis=0)\n",
    "        te_grouped_sum.drop(columns = ['css'], inplace = True)\n",
    "    elif sentiment_type == \"only_senti\":\n",
    "        te_grouped_sum = te_grouped_sum[['css']]\n",
    "    elif sentiment_type == \"no_senti\":\n",
    "        te_grouped_sum.drop(columns = ['css'], inplace = True)\n",
    "    \n",
    "    return topics, Topic_model, tr_grouped_sum, te_grouped_sum\n",
    "    \n",
    "\n",
    "def linear_regression(tr_grouped_sum, te_grouped_sum, insampler2_list, outsampler2_list):\n",
    "    \n",
    "    # Linear regression for insample R2\n",
    "    tr_X = np.array(tr_grouped_sum)\n",
    "    tr_ret = [ind[2] for ind in list(tr_grouped_sum.index)]\n",
    "    tr_Y = np.array(tr_ret).reshape(-1,1)\n",
    "    tr_regression = LinearRegression(fit_intercept=True)\n",
    "    tr_regression.fit(tr_X,tr_Y)\n",
    "    tr_Y_pred = tr_regression.predict(tr_X)\n",
    "    tr_Y_mean = np.mean(tr_Y)\n",
    "    tr_SS_tot = np.sum((tr_Y - tr_Y_mean) ** 2)\n",
    "    tr_SS_res = np.sum((tr_Y - tr_Y_pred) ** 2)\n",
    "    tr_r2 = 1 - (tr_SS_res / tr_SS_tot)\n",
    "    insampler2_list.append(tr_r2)\n",
    "\n",
    "    # Linear regression for outsample R2\n",
    "    te_X = np.array(te_grouped_sum)\n",
    "    te_ret = [ind[2] for ind in list(te_grouped_sum.index)]\n",
    "    te_Y = np.array(te_ret).reshape(-1,1)\n",
    "    te_Y_pred = tr_regression.predict(te_X)\n",
    "    te_SS_tot = np.sum((te_Y - tr_Y_mean) ** 2)\n",
    "    te_SS_res = np.sum((te_Y - te_Y_pred) ** 2)\n",
    "    te_r2 = 1 - (te_SS_res / te_SS_tot)\n",
    "    outsampler2_list.append(te_r2)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Compute the model diversity score\n",
    "def compute_model_diversity(topics):\n",
    "    unique_words = set()\n",
    "    total_words = 0\n",
    "    for topic in topics:\n",
    "        total_words += len(topic)\n",
    "        for words in topic:\n",
    "            unique_words.add(words)\n",
    "    diversity_score = len(unique_words) / total_words\n",
    "    \n",
    "    return diversity_score\n",
    "\n",
    "\n",
    "# Calculate all the scores\n",
    "def calculate_score(Topic_model, embedding_model, topics):\n",
    "    topic_info = Topic_model.get_topics()\n",
    "    topic_words = {topic: [word for word, _ in words] for topic, words in topic_info.items()}\n",
    "\n",
    "    # Get embeddings for the words in each topic\n",
    "    topic_embeddings = {}\n",
    "    for topic, words in topic_words.items():\n",
    "        embeddings = embedding_model.encode(words)\n",
    "        topic_embeddings[topic] = embeddings\n",
    "\n",
    "\n",
    "    # Calculate the intra-topic similarity (similarity between words within the same topic)\n",
    "    in_topic_similarity = {}  # Dictionary to store intra-topic similarity scores for each topic\n",
    "    # Loop through each topic and its word embeddings\n",
    "    for topic, embeddings in topic_embeddings.items():\n",
    "        distances = 0  # Initialize a variable to accumulate the cosine similarity between word pairs\n",
    "        for i in range(len(embeddings)): # Loop through all pairs of word embeddings within the topic\n",
    "            for j in range(i+1, len(embeddings)):\n",
    "                distances += cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))\n",
    "        distances /= (len(embeddings) - 0) * (len(embeddings) - 1) / 2\n",
    "        in_topic_similarity[topic] = distances[0][0]\n",
    "    in_topic_similarity = np.mean(list(in_topic_similarity.values())) # Calculate the mean intra-topic similarity across all topics\n",
    "    # for topic, score in in_topic_similarity.items():\n",
    "    #     print(f\"Topic {topic}: Mean Cosine Distance = {score}\")\n",
    "\n",
    "\n",
    "    # Calculate the between-topic similarity (similarity between different topics)\n",
    "    distances = 0\n",
    "    count = 0\n",
    "    for topic1, topic2 in combinations(topic_words, 2):\n",
    "        # Compute the centroid (average embedding) for each topic by averaging the word embeddings\n",
    "        for i in range(len(topic_embeddings[topic1])):\n",
    "            for j in range(len(topic_embeddings[topic2])):\n",
    "                distances += cosine_similarity(topic_embeddings[topic1][i].reshape(1, -1), topic_embeddings[topic2][j].reshape(1, -1))\n",
    "                count += 1\n",
    "\n",
    "    btn_topic_similarity = distances[0][0] / count\n",
    "\n",
    "        \n",
    "    # Calculate Model Diversity Score\n",
    "    topic_words_list = [[words for words, _ in Topic_model.get_topic(topic)] \n",
    "                for topic in range(len(set(topics))-1)]\n",
    "    model_diversity = compute_model_diversity(topic_words_list)\n",
    "    \n",
    "    return in_topic_similarity, btn_topic_similarity, model_diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model and Calculating R-Square and Model Evaluation Score\n",
    "Choosing 'model type', topic number', data type', and whther save model or not. \\\n",
    "Building up model combination, split the train and test data. Training the model with dataset, generating topics. \\\n",
    "Using linear regression to calculate the in-sample and out-sample R-Square for the topic weight of positive, negative, and neutral topic models.\n",
    "Calculating cosine similarity within and between topics, and model diversity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insampler2_dict = {}\n",
    "outsampler2_dict = {}\n",
    "cos_in_topic_dict = {}\n",
    "cos_bet_topic_dict = {}\n",
    "model_diversity_dict = {}\n",
    "\n",
    "modeltype = 'pcakmeans' # umaphdbscan, pcakmeans, umapgmm, pcagmm, svdkmeans, pcahdbscan\n",
    "models = 'three_models' # three_models or one_model\n",
    "combine = True\n",
    "sentiment_type = 'no_senti' # no_senti, with_senti, per_topic, per_ret, only_senti\n",
    "topic_num = 120\n",
    "save_model = False\n",
    "reduce_outliers = False\n",
    "datatype = 'contem' # contem or future\n",
    "year_num = 1\n",
    "year_list = range(2023, 2024)\n",
    "df_folder = f\"/shared/share_tm-finance/Final/Processed_df/{year_num}_year_window\"\n",
    "embeddings_folder = f\"/shared/share_tm-finance/Final/Embeddings/{year_num}_year_window\"\n",
    "saved_model_folder = f\"/shared/share_tm-finance/Final/Stored_model/{year_num}_year_window/one_model/{modeltype}\"\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "first_count = 1\n",
    "last_count = 5\n",
    "div = last_count - first_count + 1\n",
    "\n",
    "# Results from the hyperparameter tuning script\n",
    "if topic_num == 120:\n",
    "    min_cluster_size_list = [210, 230, 235, 215, 205, 220, 220, 205, 240, 230]\n",
    "elif topic_num == 60:\n",
    "    min_cluster_size_list = [350, 340, 345, 410, 365, 400, 360, 400, 410, 425]\n",
    "\n",
    "pos_min_cluster_size_list = [205, 200, 210, 220, 200, 210, 210, 220, 200, 200]\n",
    "neg_min_cluster_size_list = [200, 150, 200, 200, 220, 205, 180, 220, 225, 210]\n",
    "neu_min_cluster_size_list = [200, 200, 200, 200, 200, 200, 190, 200, 195, 185]\n",
    "\n",
    "for i in year_list:\n",
    "    \n",
    "    tar_year = i\n",
    "    df = pd.read_csv(df_folder+f'/{datatype}_{tar_year - year_num + 1}_{tar_year}.csv')\n",
    "    red_headlines = df.vocab_con_headline.tolist()\n",
    "    embeddings = np.load(embeddings_folder+f\"/{datatype}_{tar_year - year_num + 1}_{tar_year}_embeddings.npy\")\n",
    "    indices = np.arange(len(red_headlines))\n",
    "    pos_min_cluster_size = pos_min_cluster_size_list[year_list.index(i)]\n",
    "    neg_min_cluster_size = neg_min_cluster_size_list[year_list.index(i)]\n",
    "    neu_min_cluster_size = neu_min_cluster_size_list[year_list.index(i)]\n",
    "    hdb_min_cluster_size = min_cluster_size_list[year_list.index(i)]\n",
    "    pos_insampler2_list = []\n",
    "    pos_outsampler2_list = []\n",
    "    neg_insampler2_list = []\n",
    "    neg_outsampler2_list = []\n",
    "    neu_insampler2_list = []\n",
    "    neu_outsampler2_list = []\n",
    "    insampler2_list = []\n",
    "    outsampler2_list = []\n",
    "\n",
    "    #split the data into 3 part, positive css, negative css, neutral css\n",
    "    pos_indices = df[df['css'] > 0].index\n",
    "    neg_indices = df[df['css'] < 0].index\n",
    "    neu_indices = df[df['css'] == 0].index\n",
    "    pos_df = df.iloc[pos_indices,:]\n",
    "    neg_df = df.iloc[neg_indices,:]\n",
    "    neu_df = df.iloc[neu_indices,:]\n",
    "    pos_headlines = [red_headlines[ind] for ind in pos_indices]\n",
    "    neg_headlines = [red_headlines[ind] for ind in neg_indices]\n",
    "    neu_headlines = [red_headlines[ind] for ind in neu_indices]\n",
    "    pos_embeddings = embeddings[pos_indices,:]\n",
    "    neg_embeddings = embeddings[neg_indices,:]\n",
    "    neu_embeddings = embeddings[neu_indices,:]\n",
    "    \n",
    "    cos_in_topic_pos_sum = 0\n",
    "    cos_bet_topic_pos_sum = 0\n",
    "    model_diversity_pos_sum = 0\n",
    "    cos_in_topic_neg_sum = 0\n",
    "    cos_bet_topic_neg_sum = 0\n",
    "    model_diversity_neg_sum = 0\n",
    "    cos_in_topic_neu_sum = 0\n",
    "    cos_bet_topic_neu_sum = 0\n",
    "    model_diversity_neu_sum = 0\n",
    "\n",
    "    #set pos_cluster_num, neg_cluster_num, neu_cluster_num based on the number of embeddings\n",
    "    pos_cluster_num = int(topic_num * len(pos_embeddings) / len(embeddings))\n",
    "    neg_cluster_num = int(topic_num * len(neg_embeddings) / len(embeddings))\n",
    "    neu_cluster_num = int(topic_num * len(neu_embeddings) / len(embeddings))\n",
    "    diff = topic_num - (pos_cluster_num + neg_cluster_num + neu_cluster_num)\n",
    "    if pos_cluster_num < neg_cluster_num and pos_cluster_num < neu_cluster_num:\n",
    "        pos_cluster_num += diff\n",
    "    elif neg_cluster_num < pos_cluster_num and neg_cluster_num < neu_cluster_num:\n",
    "        neg_cluster_num += diff\n",
    "    else:\n",
    "        neu_cluster_num += diff\n",
    "\n",
    "    print(\"pos_cluster_num = \", pos_cluster_num)\n",
    "    print(\"neg_cluster_num = \", neg_cluster_num)\n",
    "    print(\"neu_cluster_num = \", neu_cluster_num)\n",
    "    \n",
    "    for i in range(first_count, last_count+1):\n",
    "        saved_model_folder = f\"/shared/share_tm-finance/Final/Stored_model/{year_num}_year_window/three_models/{modeltype}/pos\"\n",
    "        pos_topics, pos_Topic_model, pos_tr_grouped_sum, pos_te_grouped_sum = \\\n",
    "                train_model(saved_model_folder, pos_df, pos_headlines, pos_embeddings, \n",
    "                            modeltype, topic_num, pos_cluster_num, hdb_min_cluster_size, tar_year, save_model, reduce_outliers, i)\n",
    "        mean_score, ave_sim, model_diversity = calculate_score(pos_Topic_model, embedding_model, pos_topics)\n",
    "        cos_in_topic_pos_sum += mean_score\n",
    "        cos_bet_topic_pos_sum += ave_sim\n",
    "        model_diversity_pos_sum += model_diversity\n",
    "\n",
    "        saved_model_folder = f\"/shared/share_tm-finance/Final/Stored_model/{year_num}_year_window/three_models/{modeltype}/neg\"\n",
    "        neg_topics, neg_Topic_model, neg_tr_grouped_sum, neg_te_grouped_sum = \\\n",
    "                train_model(saved_model_folder, neg_df, neg_headlines, neg_embeddings,\n",
    "                            modeltype, topic_num, neg_cluster_num, hdb_min_cluster_size, tar_year, save_model, reduce_outliers, i)\n",
    "        mean_score, ave_sim, model_diversity = calculate_score(neg_Topic_model, embedding_model, neg_topics)\n",
    "        cos_in_topic_neg_sum += mean_score\n",
    "        cos_bet_topic_neg_sum += ave_sim\n",
    "        model_diversity_neg_sum += model_diversity\n",
    "\n",
    "        saved_model_folder = f\"/shared/share_tm-finance/Final/Stored_model/{year_num}_year_window/three_models/{modeltype}/neu\"\n",
    "        neu_topics, neu_Topic_model, neu_tr_grouped_sum, neu_te_grouped_sum = \\\n",
    "                train_model(saved_model_folder, neu_df, neu_headlines, neu_embeddings,\n",
    "                            modeltype, topic_num, neu_cluster_num, hdb_min_cluster_size, tar_year, save_model, reduce_outliers, i)\n",
    "        mean_score, ave_sim, model_diversity = calculate_score(neu_Topic_model, embedding_model, neu_topics)\n",
    "        cos_in_topic_neu_sum += mean_score\n",
    "        cos_bet_topic_neu_sum += ave_sim\n",
    "        model_diversity_neu_sum += model_diversity\n",
    "        \n",
    "        if combine:   \n",
    "            #get the last column name of the last column of pos_tr_grouped_sum\n",
    "            pos_last_col = int(pos_tr_grouped_sum.columns[-1])\n",
    "            neg_tr_grouped_sum.columns = [str(int(col) + pos_last_col) for col in neg_tr_grouped_sum.columns]\n",
    "            neg_last_col = int(neg_tr_grouped_sum.columns[-1])\n",
    "            neu_tr_grouped_sum.columns = [str(int(col) + neg_last_col) for col in neu_tr_grouped_sum.columns]\n",
    "            \n",
    "            pos_last_col = int(pos_te_grouped_sum.columns[-1])\n",
    "            neg_te_grouped_sum.columns = [str(int(col) + pos_last_col) for col in neg_te_grouped_sum.columns]\n",
    "            neg_last_col = int(neg_te_grouped_sum.columns[-1])\n",
    "            neu_te_grouped_sum.columns = [str(int(col) + neg_last_col) for col in neu_te_grouped_sum.columns]\n",
    "\n",
    "            tr_grouped_sum = pd.concat([pos_tr_grouped_sum, neg_tr_grouped_sum, neu_tr_grouped_sum], axis = 1)\n",
    "            tr_grouped_sum.fillna(0, inplace = True)\n",
    "            te_grouped_sum = pd.concat([pos_te_grouped_sum, neg_te_grouped_sum, neu_te_grouped_sum], axis = 1)\n",
    "            te_grouped_sum.fillna(0, inplace = True)\n",
    "            linear_regression(tr_grouped_sum, te_grouped_sum, insampler2_list, outsampler2_list)\n",
    "            \n",
    "        else:\n",
    "            linear_regression(pos_tr_grouped_sum, pos_te_grouped_sum, pos_insampler2_list, pos_outsampler2_list)\n",
    "            linear_regression(neg_tr_grouped_sum, neg_te_grouped_sum, neg_insampler2_list, neg_outsampler2_list)\n",
    "            linear_regression(neu_tr_grouped_sum, neu_te_grouped_sum, neu_insampler2_list, neu_outsampler2_list)\n",
    "            \n",
    "                                                                                                                                                                    \n",
    "\n",
    "    if combine:\n",
    "        insampler2_dict[tar_year] = np.mean(insampler2_list)\n",
    "        outsampler2_dict[tar_year] = np.mean(outsampler2_list)\n",
    "    else:\n",
    "        sep_insampler2_list = (pos_insampler2_list* len(pos_embeddings) / len(embeddings)) + (neg_insampler2_list* len(neg_embeddings) / len(embeddings)) + (neu_insampler2_list* len(neu_embeddings) / len(embeddings))\n",
    "        sep_outsampler2_list = (pos_outsampler2_list* len(pos_embeddings) / len(embeddings)) + (neg_outsampler2_list* len(neg_embeddings) / len(embeddings)) + (neu_outsampler2_list* len(neu_embeddings) / len(embeddings))\n",
    "        insampler2_dict[tar_year] = np.mean(sep_insampler2_list)\n",
    "        insampler2_dict[tar_year] = np.mean(sep_outsampler2_list)\n",
    "\n",
    "    cos_in_topic_dict[tar_year] = (cos_in_topic_pos_sum* len(pos_embeddings) / len(embeddings) + cos_in_topic_neg_sum* len(neg_embeddings) / len(embeddings) + cos_in_topic_neu_sum* len(neu_embeddings) / len(embeddings)) / div\n",
    "    cos_bet_topic_dict[tar_year] = (cos_bet_topic_pos_sum* len(pos_embeddings) / len(embeddings) + cos_bet_topic_neg_sum* len(neg_embeddings) / len(embeddings) + cos_bet_topic_neu_sum* len(neu_embeddings) / len(embeddings)) / div\n",
    "    model_diversity_dict[tar_year] = (model_diversity_pos_sum* len(pos_embeddings) / len(embeddings) + model_diversity_neg_sum* len(neg_embeddings) / len(embeddings) + model_diversity_neu_sum* len(neu_embeddings) / len(embeddings)) / div\n",
    "    \n",
    "    print(\"Year {year} is done\".format(year = tar_year))\n",
    "        \n",
    "if not combine:\n",
    "    print(\"sep_insample = \", insampler2_dict)\n",
    "    print(\"sep_outsample = \", insampler2_dict)\n",
    "else:\n",
    "    print(\"insample = \", insampler2_dict)\n",
    "    print(\"outsample = \", outsampler2_dict)\n",
    "\n",
    "print(\"cos_in_topic = \", cos_in_topic_dict)\n",
    "print(\"cos_bet_topic = \", cos_bet_topic_dict)\n",
    "print(\"model_diversity = \", model_diversity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_grouped_sum2 = tr_grouped_sum.copy()\n",
    "tr_grouped_sum2 = tr_grouped_sum2.groupby('comnam').sum()\n",
    "tr_grouped_sum2.reset_index(level=0, inplace=True)\n",
    "tr_grouped_sum2 = tr_grouped_sum2.T\n",
    "tr_grouped_sum2.reset_index(drop = True, inplace = True)\n",
    "tr_grouped_sum2.columns = tr_grouped_sum2.iloc[0]\n",
    "tr_grouped_sum2 = tr_grouped_sum2.drop(tr_grouped_sum2.index[0])\n",
    "tr_grouped_sum2.reset_index(drop = True, inplace = True)\n",
    "\n",
    "pos_df = pos_Topic_model.get_topic_info()\n",
    "pos_df.drop(columns = ['Topic'], inplace = True)\n",
    "neg_df = neg_Topic_model.get_topic_info()\n",
    "neg_df.drop(columns = ['Topic'], inplace = True)\n",
    "neu_df = neu_Topic_model.get_topic_info()\n",
    "neu_df.drop(columns = ['Topic'], inplace = True)\n",
    "topic_info = pd.concat([pos_df, neg_df, neu_df], axis = 0)\n",
    "topic_info.reset_index(drop = True, inplace = True)\n",
    "\n",
    "topic_exposure = pd.concat([tr_grouped_sum2, topic_info], axis = 1)\n",
    "\n",
    "data = topic_exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_topic_info(topic_number):\n",
    "    \"\"\"Fetch and plot topic representation and company exposures for a given topic number.\n",
    "       Also, list companies with exposure greater than 100.\"\"\"\n",
    "    # Filter row by topic number\n",
    "    topic_row = data[data.index == topic_number]\n",
    "    \n",
    "    if topic_row.empty:\n",
    "        print(\"Topic number not found.\")\n",
    "        return\n",
    "    \n",
    "    # Extract topic representation\n",
    "    representation = topic_row['Representation'].values[0]\n",
    "    print(f\"Representation of Topic {topic_number}: {representation}\")\n",
    "    \n",
    "    # Extract company exposures and filter for companies with exposure > 100\n",
    "    exposures = topic_row.iloc[:, :-4].T  # All columns except the last four\n",
    "    exposures.columns = ['Exposure']\n",
    "    \n",
    "    # Identify companies with exposure greater than 100\n",
    "    high_exposure_companies = exposures[exposures['Exposure'] > 100]\n",
    "    if not high_exposure_companies.empty:\n",
    "        print(\"\\nCompanies with exposure greater than 100:\")\n",
    "        for company in high_exposure_companies.index:\n",
    "            print(f\"- {company}: {high_exposure_companies.loc[company, 'Exposure']}\")\n",
    "    else:\n",
    "        print(\"No companies with exposure greater than 100 for this topic.\")\n",
    "    \n",
    "    # Plot exposures without x-axis numbers\n",
    "    plt.figure(figsize=(14, 8))  # Make the plot bigger\n",
    "    plt.bar(data.columns[:-4], topic_row.iloc[:, :-4].values[0])\n",
    "    plt.xticks([])  # Hide x-axis numbers\n",
    "    plt.xlabel(\"Company Number\")\n",
    "    plt.ylabel(\"Exposure Level\")\n",
    "    plt.show()\n",
    "\n",
    "    return exposures\n",
    "\n",
    "def plot_company_exposure(company_name):\n",
    "    \"\"\"Plot exposure levels of a specific company across all topics and display the representation of the highest exposure topic.\"\"\"\n",
    "    if company_name not in data.columns:\n",
    "        print(\"Company not found.\")\n",
    "        return\n",
    "    \n",
    "    # Extract exposures for the company across all topics\n",
    "    exposures = data[[company_name]]\n",
    "    exposures.columns = ['Exposure']\n",
    "    \n",
    "    # Find the topic with the highest exposure for this company\n",
    "    max_exposure_topic = exposures['Exposure'].idxmax()\n",
    "    max_exposure_value = exposures['Exposure'].max()\n",
    "    highest_topic_representation = data.loc[max_exposure_topic, 'Representation']\n",
    "    \n",
    "    # Display the representation of the topic with the highest exposure\n",
    "    print(f\"Highest Exposure Topic for {company_name}: Topic {max_exposure_topic}\")\n",
    "    print(f\"Exposure Level: {max_exposure_value}\")\n",
    "    print(f\"Representation: {highest_topic_representation}\")\n",
    "    \n",
    "    # Plot exposures for all topics\n",
    "    exposures.plot(kind='bar', legend=False, title=f\"{company_name} Exposure to All Topics\")\n",
    "    plt.xlabel(\"Topic Number\")\n",
    "    plt.ylabel(\"Exposure Level\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposures = get_topic_info(17)  # Replace 0 with the desired topic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_company_exposure(\"AMAZON COM INC\")  # Replace \"3M CO\" with the desired company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{topic_num}_Data.xlsx'\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    "program_list = [insampler2_dict, outsampler2_dict, cos_in_topic_dict, cos_bet_topic_dict, model_diversity_dict]\n",
    "data_list = ['Insample_R2', 'Outsample_R2', 'Cos_in_topic', 'Cos_btn_topic', 'Diversity']\n",
    "\n",
    "for i in range(len(data_list)):\n",
    "    ws = wb[data_list[i]]\n",
    "    for cell in ws['A']:\n",
    "        if cell.value == models:\n",
    "            for row_num in range(cell.row, cell.row + 17):\n",
    "                sentiment_type_cell = ws.cell(row=row_num, column=2)\n",
    "                if sentiment_type_cell.value == sentiment_type:\n",
    "                    for row_num in range(sentiment_type_cell.row, sentiment_type_cell.row + 4):\n",
    "                        model_cell = ws.cell(row=row_num, column=3).value\n",
    "                        if model_cell == f\"{modeltype}_{topic_num}\":\n",
    "                            for year, value in program_list[i].items():\n",
    "                                ws.cell(row=row_num, column=year - 2013 + 3, value=value)\n",
    "                    break\n",
    "wb.save(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
