{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/rl3444/.conda/envs/NLP_Env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "import datetime as dt\n",
    "import pandas_market_calendars as mcal\n",
    "import re\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "db = wrds.Connection(wrds_username = \"kevinlin5549\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SP500_CRSP_data(year_range):\n",
    "    first_year, last_year = str(year_range[0]),str(year_range[1])\n",
    "    sp_500_query = f\"\"\"SELECT a.*, b.date, b.ret, b.prc, b.openprc\n",
    "                        FROM crsp.dsp500list as a,\n",
    "                        crsp.dsf as b\n",
    "                        WHERE a.permno=b.permno\n",
    "                        and b.date >= a.start and b.date<= a.ending\n",
    "                        and b.date>='01/01/{first_year}' and b.date<='12/31/{last_year}'\n",
    "                        order by date;\"\"\"\n",
    "    sp_500 = db.raw_sql(sp_500_query,date_cols=['start', 'ending', 'date'])\n",
    "    dse = db.raw_sql(\"\"\"\n",
    "                        select comnam,ncusip, namedt, nameendt,permno\n",
    "                        from crsp.dsenames\n",
    "                        \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "    dse['nameendt']=dse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "    sp500_full = pd.merge(sp_500, dse, how = 'left', on = 'permno')\n",
    "    sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                                & (sp500_full.date<=sp500_full.nameendt)]\n",
    "    sp500_full.reset_index(inplace = True,drop = True)\n",
    "    sp500_full = sp500_full[[\"permno\",\"date\",\"comnam\",\"ret\",\"openprc\",\"prc\"]]\n",
    "    sp500_full[\"prc\"] = sp500_full[\"prc\"].apply(abs)\n",
    "    sp500_full[\"CO_ret\"] = (sp500_full['prc'] - sp500_full['openprc'])/sp500_full['openprc']\n",
    "    return sp500_full\n",
    "\n",
    "mapping_file = pd.read_csv(\"SP500_Mapping_file.csv\")\n",
    "SP500_entity_id_str = ','.join(f\"'{id}'\" for id in list(mapping_file.rp_entity_id)) \n",
    "mapping_file.head()\n",
    "\n",
    "def SP500_RP_headline_data(year_range):\n",
    "    years = range(int(year_range[0]),int(year_range[-1])+1)\n",
    "    years_str = [str(year) for year in years]\n",
    "    RP_df = pd.DataFrame()\n",
    "    for year in years_str:\n",
    "        RP_year_query =f\"\"\"\n",
    "                        SELECT DISTINCT timestamp_utc,rp_entity_id,headline,css,relevance\n",
    "                        FROM rpna.rpa_djpr_equities_{year}\n",
    "                        WHERE rp_entity_id IN ({SP500_entity_id_str})\n",
    "                        \"\"\"\n",
    "        RP_df = pd.concat((RP_df,db.raw_sql(RP_year_query)),axis = 0)\n",
    "    RP_df = RP_df.drop_duplicates((\"rp_entity_id\",\"headline\"))\n",
    "    return RP_df\n",
    "\n",
    "def contem_ret(year_range):\n",
    "    first_year,last_year = int(year_range[0]),int(year_range[-1])\n",
    "\n",
    "    # Create financial dataframe from crsp and link to entity id\n",
    "    sp500_crsp = SP500_CRSP_data(year_range)\n",
    "    sp500_crsp_rpid =  sp500_crsp.merge(mapping_file, on = \"permno\", how = \"inner\")\n",
    "\n",
    "    # Create RavenPack headline data and map the timestamp to contemporaneous return date\n",
    "    sp500_rp =  SP500_RP_headline_data(year_range)\n",
    "    sp500_rp.set_index(\"timestamp_utc\",inplace= True)\n",
    "    sp500_rp[\"timestamp_NY\"] = pd.to_datetime(sp500_rp.index).tz_localize(\"UTC\").tz_convert(\"America/New_York\")\n",
    "    sp500_rp = sp500_rp.reset_index()\n",
    "    sp500_rp['index'] = sp500_rp.index\n",
    "    nyse = mcal.get_calendar(\"NYSE\")\n",
    "    nyse_tradingdays= nyse.valid_days(start_date=f\"{str(first_year)}-01-01\",end_date=f\"{str(last_year)}-12-31\")\\\n",
    "        .tz_localize(None).tz_localize(\"America/New_York\")\n",
    "    nyse_tradingdays_closing = nyse_tradingdays + dt.timedelta(hours = 16)\n",
    "    trading_days_df = pd.DataFrame({'trading_close': nyse_tradingdays_closing, 'contem_ret_date': nyse_tradingdays_closing.date})\n",
    "\n",
    "    # Use merge_asof to align the headlines with the trading close times\n",
    "    sp500_rp = pd.merge_asof(sp500_rp.sort_values('timestamp_NY'), trading_days_df,\n",
    "                            left_on='timestamp_NY', right_on='trading_close',\n",
    "                            direction='forward')\n",
    "    sp500_rp = sp500_rp.sort_values(\"index\")\n",
    "    sp500_rp\n",
    "    sp500_rp = sp500_rp[['timestamp_utc', 'rp_entity_id', 'headline', 'css', 'relevance', 'timestamp_NY', 'contem_ret_date']]\n",
    "    sp500_rp = sp500_rp.reset_index()\n",
    "    sp500_rp.drop(columns = [\"index\"],inplace = True)\n",
    "\n",
    "    # Merge crsp dataframe with RP dataframe\n",
    "    sp500_rp_contem_ret = sp500_rp[[\"contem_ret_date\",\"rp_entity_id\",\"headline\", \"css\", \"relevance\"]]\n",
    "    sp500_rp_contem_ret = sp500_rp_contem_ret.dropna()\n",
    "    sp500_rp_contem_ret.contem_ret_date = pd.to_datetime(sp500_rp_contem_ret.contem_ret_date)\n",
    "    sp500_crsp_rpid = sp500_crsp_rpid[[\"date\",\"rp_entity_id\",\"comnam\",\"ret\"]]\n",
    "    sp500_crsp_rp_contem_ret = pd.merge(sp500_crsp_rpid,sp500_rp_contem_ret,left_on=[\"date\",\"rp_entity_id\"],\\\n",
    "                                        right_on=[\"contem_ret_date\",\"rp_entity_id\"],how = \"inner\").drop(columns = \"contem_ret_date\")\n",
    "    \n",
    "    # sp500_crsp_rp_contem_ret.drop_duplicates((\"rp_entity_id\",\"headline\"),inplace=True)\n",
    "    sp500_crsp_rp_contem_ret.dropna(inplace=True)\n",
    "    return sp500_crsp_rp_contem_ret\n",
    "\n",
    "def future_ret(year_range):\n",
    "    first_year,last_year = int(year_range[0]),int(year_range[-1])\n",
    "\n",
    "    # Create financial dataframe from crsp and link to entity id\n",
    "    sp500_crsp = SP500_CRSP_data(year_range)\n",
    "    sp500_crsp_rpid =  sp500_crsp.merge(mapping_file, on = \"permno\", how = \"inner\")\n",
    "\n",
    "    # Create RavenPack headline data and map the timestamp to future return date\n",
    "    sp500_rp =  SP500_RP_headline_data(year_range)\n",
    "    sp500_rp.set_index(\"timestamp_utc\",inplace= True)\n",
    "    sp500_rp[\"timestamp_NY\"] = pd.to_datetime(sp500_rp.index).tz_localize(\"UTC\").tz_convert(\"America/New_York\")\n",
    "    nyse = mcal.get_calendar(\"NYSE\")\n",
    "    nyse_tradingdays= nyse.valid_days(start_date=f\"{str(first_year)}-01-01\",end_date=f\"{str(last_year)}-12-31\")\\\n",
    "        .tz_localize(None).tz_localize(\"America/New_York\")\n",
    "    nyse_tradingdays_opening = nyse_tradingdays + dt.timedelta(hours = 9)\n",
    "    nyse_tradingdays_closing = nyse_tradingdays + dt.timedelta(hours = 16)\n",
    "    def future_ret_date(timestamp):\n",
    "        later_opening = nyse_tradingdays_opening[nyse_tradingdays_opening>=timestamp]\n",
    "        later_closing = nyse_tradingdays_closing[nyse_tradingdays_closing>=timestamp]\n",
    "        if (not later_opening.empty) & (not later_closing.empty):\n",
    "            next_opening = later_opening[0]\n",
    "            next_closing = later_closing[0]\n",
    "            if next_opening.date() == next_closing.date():\n",
    "                return [next_opening.date(),1]\n",
    "            else: \n",
    "                return [next_opening.date(),0]\n",
    "        else:\n",
    "            return [None,None]\n",
    "    sp500_rp_future_ret_date = sp500_rp.apply(lambda row:future_ret_date(row['timestamp_NY']),axis = 1, result_type=\"expand\")\n",
    "    sp500_rp_future_ret_date = sp500_rp_future_ret_date.rename(columns = {0:\"future_ret_date\",1:\"bool_CO_ret\"}) \n",
    "    sp500_rp = pd.concat([sp500_rp,sp500_rp_future_ret_date],axis = 1)\n",
    "\n",
    "    # Merge crsp dataframe with RP dataframe\n",
    "    sp500_rp.future_ret_date = pd.to_datetime(sp500_rp.future_ret_date)\n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rpid.merge(sp500_rp,left_on=[\"date\",\"rp_entity_id\"],right_on = [\"future_ret_date\",\"rp_entity_id\"],how = \"inner\")\n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rp_future_ret[[\"date\",\"rp_entity_id\",\"comnam\",\"CO_ret\",\"ret\",\"headline\",\"bool_CO_ret\"]]\n",
    "    \n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rp_future_ret.drop_duplicates(subset = [\"rp_entity_id\",\"headline\"])\n",
    "    sp500_crsp_rp_future_ret[\"future_ret\"] = sp500_crsp_rp_future_ret.apply(lambda row: row[\"CO_ret\"] if row[\"bool_CO_ret\"] \\\n",
    "                                                                            else (row[\"ret\"] if not row[\"bool_CO_ret\"]  else None), axis = 1)\n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rp_future_ret.drop(columns = ['CO_ret','ret','bool_CO_ret'])\n",
    "    # sp500_crsp_rp_future_ret.drop_duplicates(inplace = True)\n",
    "    sp500_crsp_rp_future_ret.dropna(inplace = True)\n",
    "    \n",
    "    return sp500_crsp_rp_future_ret\n",
    "\n",
    "# Remove individual commenter first\n",
    "def remove_useless(headline):\n",
    "    end_pattern = re.search(r\" (-+)? By|-[0-9]+-|- ?\\b[A-Z][a-z]{2}\\b \\d{1,2}|(null)|--?\\s\\w{1,}(\\s\\w{1,})?$|-- Barrons.com|researchandmarkets.com|by\\s\\w{1,}(\\s\\w{1,})?$|>[A-Z]{2,4}\", headline, flags=re.IGNORECASE)\n",
    "    if end_pattern:\n",
    "        headline = headline[:end_pattern.start()]\n",
    "    return headline\n",
    "\n",
    "def remove_number_day(headline):\n",
    "    headline = re.sub(r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\b', '', headline, flags=re.IGNORECASE)\n",
    "    headline = re.sub(r'\\bMay\\b', '', headline)\n",
    "    headline = re.sub(r'\\b(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)\\b', '', headline, flags=re.IGNORECASE)\n",
    "    headline = re.sub(r'\\b(Sunday|sunday|Sun)\\b', '', headline)\n",
    "    headline = re.sub(r\"\\d*\\.?\\d+[Bb]|bn\", \"bln\", headline)\n",
    "    headline = re.sub(r\"\\d*\\.?\\d+[Mm]|mn\", \"mln\", headline)\n",
    "    headline = re.sub(r\"\\d*\\.?\\d+[Kk]\", \"k\", headline)\n",
    "    headline = re.sub(r'\\b\\d+(\\.\\d+)?(?!\\s*%)\\b', '', headline)\n",
    "    return headline\n",
    "\n",
    "\n",
    "# Tokenize the headline into tokens which are alphanumeric words including period . \n",
    "def custom_tokenizer(headline):\n",
    "    tokens = re.findall(r\"\\b[a-zA-z0-9\\.][a-zA-z0-9\\.]+\\b\",headline.lower())  \n",
    "    return tokens\n",
    "\n",
    "# Remove tokens according to the principles \n",
    "def custom_processor(headline, remove_words):\n",
    "    tokens = custom_tokenizer(headline)\n",
    "    new_tokens = [token for token in tokens if token not in remove_words]\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "def get_user_selected_words(words_top, bool):\n",
    "    if bool:\n",
    "        print(\"Top words in headlines:\")\n",
    "        for word in words_top:\n",
    "            print(word)\n",
    "        print(\"\\nPlease enter the words you want to delete, separated by commas:\")\n",
    "        user_input = input().strip()\n",
    "        selected_words = [word.strip() for word in user_input.split(\",\") if word.strip() in words_top]\n",
    "    else:\n",
    "        selected_words = []\n",
    "    return selected_words\n",
    "\n",
    "def clean_heandline(df, relevance_threshold, top_words_num, select_manually = True):\n",
    "    df = df[df['relevance'] >= relevance_threshold]\n",
    "    df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n",
    "    df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"vocab_con_headline\"].apply(remove_number_day)\n",
    "\n",
    "    #clean the stop words\n",
    "    stop_words = list(ENGLISH_STOP_WORDS)\n",
    "    words_list = [\"release\", \"press\", \"pgr\", \"mw\", \"llp\", \"corp\", \"live\", \"corporation\",\"plc\", \"factset\",\n",
    "                     \"llc\", \"group\", \"target\", \"blog\", \"st\", \"chart\", \"update\", \"dir\", \"barron\", \"pbulletin\"]\n",
    "    stop_words.extend(words_list)\n",
    "    df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"vocab_con_headline\"].apply(lambda headline: custom_processor(headline, stop_words))\n",
    "\n",
    "    # Remove words that appear only once and the top 100 words\n",
    "    remove_words = set()\n",
    "    # Create a vocab dictionary with the words and their counts\n",
    "    vocab_con_headlines = df.loc[:,\"vocab_con_headline\"].tolist()\n",
    "    vocab = collections.Counter()\n",
    "    for headline in vocab_con_headlines:\n",
    "        vocab.update(custom_tokenizer(headline))\n",
    "    \n",
    "    words_once = [word for word,count in vocab.items() if count ==1]\n",
    "    remove_words.update(words_once)\n",
    "    top_count = sorted(vocab.values(),reverse = True)[top_words_num]\n",
    "    words_top = [word for word,count in vocab.items() if count >=top_count]\n",
    "    user_selected_words = get_user_selected_words(words_top, select_manually)\n",
    "    remove_words.update(user_selected_words)\n",
    "    \n",
    "    df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"vocab_con_headline\"].apply(lambda headline: custom_processor(headline, remove_words))\n",
    "    df = df.drop(columns = [\"relevance\"])\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df.vocab_con_headline != \"\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10332/10332 [03:03<00:00, 56.24it/s]\n",
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9870/9870 [02:50<00:00, 57.86it/s]\n",
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10069/10069 [02:58<00:00, 56.50it/s]\n",
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10432/10432 [03:04<00:00, 56.65it/s]\n",
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10226/10226 [02:56<00:00, 57.98it/s]\n",
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10792/10792 [03:06<00:00, 57.87it/s]\n",
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12393/12393 [03:35<00:00, 57.41it/s]\n",
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12172/12172 [03:33<00:00, 57.08it/s] \n",
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12318/12318 [03:39<00:00, 56.19it/s]\n",
      "/tmp/ipykernel_1017827/3073605661.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_useless)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12967/12967 [03:53<00:00, 55.44it/s]\n"
     ]
    }
   ],
   "source": [
    "df_folder = '/shared/share_tm-finance'\n",
    "datatype = \"contem\"\n",
    "relevance_threshold = 75\n",
    "top_words_num = 150\n",
    "select_manually = False\n",
    "\n",
    "for i in range(2014,2024):\n",
    "    year_range = (i, i)\n",
    "    if datatype == \"contem\":\n",
    "        df = contem_ret(year_range)\n",
    "    else:\n",
    "        df = future_ret(year_range)\n",
    "\n",
    "    \n",
    "    # save processed datafram\n",
    "    df_cleaned = clean_heandline(df, relevance_threshold, top_words_num, select_manually)\n",
    "    print(\"cleaned \", i)\n",
    "    df_cleaned.to_csv(df_folder+f\"/Processed_df_Sentiment/One_year_window/{type}_{i}_senti.csv\".format(type = datatype, year = i), index = False)\n",
    "\n",
    "    #save embeddings\n",
    "    red_headlines = df_cleaned.vocab_con_headline.tolist()\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedding_model.encode(red_headlines, show_progress_bar = True)\n",
    "    embeddings = np.save(df_folder+f\"/Embeddings_with_Sentiment/One_year_window/{type}_{i}_senti_embeddings.npy\".format(type = datatype, year = i), embeddings)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_folder = '/shared/share_tm-finance'\n",
    "# datatype = \"contem\"\n",
    "# i = 2023\n",
    "# df_cleaned_grid=pd.read_csv(df_folder+\"/Processed_df_Sentiment/One_year_window/{type}_{year}_senti.csv\".format(type = datatype, year = i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
