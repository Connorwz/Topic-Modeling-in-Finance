{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/rl3444/.conda/envs/NLP_Env/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "import datetime as dt\n",
    "import pandas_market_calendars as mcal\n",
    "import re\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "db = wrds.Connection(wrds_username = \"kevinlin5549\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SP500_CRSP_data(year_range):\n",
    "    first_year, last_year = str(year_range[0]),str(year_range[1])\n",
    "    sp_500_query = f\"\"\"SELECT a.*, b.date, b.ret, b.prc, b.openprc\n",
    "                        FROM crsp.dsp500list as a,\n",
    "                        crsp.dsf as b\n",
    "                        WHERE a.permno=b.permno\n",
    "                        and b.date >= a.start and b.date<= a.ending\n",
    "                        and b.date>='01/01/{first_year}' and b.date<='12/31/{last_year}'\n",
    "                        order by date;\"\"\"\n",
    "    sp_500 = db.raw_sql(sp_500_query,date_cols=['start', 'ending', 'date'])\n",
    "    dse = db.raw_sql(\"\"\"\n",
    "                        select comnam,ncusip, namedt, nameendt,permno\n",
    "                        from crsp.dsenames\n",
    "                        \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "    dse['nameendt']=dse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "    sp500_full = pd.merge(sp_500, dse, how = 'left', on = 'permno')\n",
    "    sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                                & (sp500_full.date<=sp500_full.nameendt)]\n",
    "    sp500_full.reset_index(inplace = True,drop = True)\n",
    "    sp500_full = sp500_full[[\"permno\",\"date\",\"comnam\",\"ret\",\"openprc\",\"prc\"]]\n",
    "    sp500_full[\"prc\"] = sp500_full[\"prc\"].apply(abs)\n",
    "    sp500_full[\"CO_ret\"] = (sp500_full['prc'] - sp500_full['openprc'])/sp500_full['openprc']\n",
    "    return sp500_full\n",
    "\n",
    "mapping_file = pd.read_csv(\"SP500_Mapping_file.csv\")\n",
    "SP500_entity_id_str = ','.join(f\"'{id}'\" for id in list(mapping_file.rp_entity_id)) \n",
    "mapping_file.head()\n",
    "\n",
    "def SP500_RP_headline_data(year_range):\n",
    "    years = range(int(year_range[0]),int(year_range[-1])+1)\n",
    "    years_str = [str(year) for year in years]\n",
    "    RP_df = pd.DataFrame()\n",
    "    for year in years_str:\n",
    "        RP_year_query =f\"\"\"\n",
    "                        SELECT DISTINCT timestamp_utc,rp_entity_id,headline,css\n",
    "                        FROM rpna.rpa_djpr_equities_{year}\n",
    "                        WHERE rp_entity_id IN ({SP500_entity_id_str})\n",
    "                        \"\"\"\n",
    "        RP_df = pd.concat((RP_df,db.raw_sql(RP_year_query)),axis = 0)\n",
    "    RP_df = RP_df.drop_duplicates((\"rp_entity_id\",\"headline\"))\n",
    "    return RP_df\n",
    "\n",
    "def contem_ret(year_range):\n",
    "    first_year,last_year = int(year_range[0]),int(year_range[-1])\n",
    "\n",
    "    # Create financial dataframe from crsp and link to entity id\n",
    "    sp500_crsp = SP500_CRSP_data(year_range)\n",
    "    sp500_crsp_rpid =  sp500_crsp.merge(mapping_file, on = \"permno\", how = \"inner\")\n",
    "\n",
    "    # Create RavenPack headline data and map the timestamp to contemporaneous return date\n",
    "    sp500_rp =  SP500_RP_headline_data(year_range)\n",
    "    sp500_rp.set_index(\"timestamp_utc\",inplace= True)\n",
    "    sp500_rp[\"timestamp_NY\"] = pd.to_datetime(sp500_rp.index).tz_localize(\"UTC\").tz_convert(\"America/New_York\")\n",
    "    sp500_rp = sp500_rp.reset_index()\n",
    "    sp500_rp['index'] = sp500_rp.index\n",
    "    nyse = mcal.get_calendar(\"NYSE\")\n",
    "    nyse_tradingdays= nyse.valid_days(start_date=f\"{str(first_year)}-01-01\",end_date=f\"{str(last_year)}-12-31\")\\\n",
    "        .tz_localize(None).tz_localize(\"America/New_York\")\n",
    "    nyse_tradingdays_closing = nyse_tradingdays + dt.timedelta(hours = 16)\n",
    "    trading_days_df = pd.DataFrame({'trading_close': nyse_tradingdays_closing, 'contem_ret_date': nyse_tradingdays_closing.date})\n",
    "\n",
    "    # Use merge_asof to align the headlines with the trading close times\n",
    "    sp500_rp = pd.merge_asof(sp500_rp.sort_values('timestamp_NY'), trading_days_df,\n",
    "                            left_on='timestamp_NY', right_on='trading_close',\n",
    "                            direction='forward')\n",
    "    sp500_rp = sp500_rp.sort_values(\"index\")\n",
    "    sp500_rp\n",
    "    sp500_rp = sp500_rp[['timestamp_utc', 'rp_entity_id', 'headline', 'css', 'timestamp_NY', 'contem_ret_date']]\n",
    "    sp500_rp = sp500_rp.reset_index()\n",
    "    sp500_rp.drop(columns = [\"index\"],inplace = True)\n",
    "\n",
    "    # Merge crsp dataframe with RP dataframe\n",
    "    sp500_rp_contem_ret = sp500_rp[[\"contem_ret_date\",\"rp_entity_id\",\"headline\", \"css\"]]\n",
    "    sp500_rp_contem_ret = sp500_rp_contem_ret.dropna()\n",
    "    sp500_rp_contem_ret.contem_ret_date = pd.to_datetime(sp500_rp_contem_ret.contem_ret_date)\n",
    "    sp500_crsp_rpid = sp500_crsp_rpid[[\"date\",\"rp_entity_id\",\"comnam\",\"ret\"]]\n",
    "    sp500_crsp_rp_contem_ret = pd.merge(sp500_crsp_rpid,sp500_rp_contem_ret,left_on=[\"date\",\"rp_entity_id\"],\\\n",
    "                                        right_on=[\"contem_ret_date\",\"rp_entity_id\"],how = \"inner\").drop(columns = \"contem_ret_date\")\n",
    "    \n",
    "    # sp500_crsp_rp_contem_ret.drop_duplicates((\"rp_entity_id\",\"headline\"),inplace=True)\n",
    "    sp500_crsp_rp_contem_ret.dropna(inplace=True)\n",
    "    return sp500_crsp_rp_contem_ret\n",
    "\n",
    "def future_ret(year_range):\n",
    "    first_year,last_year = int(year_range[0]),int(year_range[-1])\n",
    "\n",
    "    # Create financial dataframe from crsp and link to entity id\n",
    "    sp500_crsp = SP500_CRSP_data(year_range)\n",
    "    sp500_crsp_rpid =  sp500_crsp.merge(mapping_file, on = \"permno\", how = \"inner\")\n",
    "\n",
    "    # Create RavenPack headline data and map the timestamp to future return date\n",
    "    sp500_rp =  SP500_RP_headline_data(year_range)\n",
    "    sp500_rp.set_index(\"timestamp_utc\",inplace= True)\n",
    "    sp500_rp[\"timestamp_NY\"] = pd.to_datetime(sp500_rp.index).tz_localize(\"UTC\").tz_convert(\"America/New_York\")\n",
    "    nyse = mcal.get_calendar(\"NYSE\")\n",
    "    nyse_tradingdays= nyse.valid_days(start_date=f\"{str(first_year)}-01-01\",end_date=f\"{str(last_year)}-12-31\")\\\n",
    "        .tz_localize(None).tz_localize(\"America/New_York\")\n",
    "    nyse_tradingdays_opening = nyse_tradingdays + dt.timedelta(hours = 9)\n",
    "    nyse_tradingdays_closing = nyse_tradingdays + dt.timedelta(hours = 16)\n",
    "    def future_ret_date(timestamp):\n",
    "        later_opening = nyse_tradingdays_opening[nyse_tradingdays_opening>=timestamp]\n",
    "        later_closing = nyse_tradingdays_closing[nyse_tradingdays_closing>=timestamp]\n",
    "        if (not later_opening.empty) & (not later_closing.empty):\n",
    "            next_opening = later_opening[0]\n",
    "            next_closing = later_closing[0]\n",
    "            if next_opening.date() == next_closing.date():\n",
    "                return [next_opening.date(),1]\n",
    "            else: \n",
    "                return [next_opening.date(),0]\n",
    "        else:\n",
    "            return [None,None]\n",
    "    sp500_rp_future_ret_date = sp500_rp.apply(lambda row:future_ret_date(row['timestamp_NY']),axis = 1, result_type=\"expand\")\n",
    "    sp500_rp_future_ret_date = sp500_rp_future_ret_date.rename(columns = {0:\"future_ret_date\",1:\"bool_CO_ret\"}) \n",
    "    sp500_rp = pd.concat([sp500_rp,sp500_rp_future_ret_date],axis = 1)\n",
    "\n",
    "    # Merge crsp dataframe with RP dataframe\n",
    "    sp500_rp.future_ret_date = pd.to_datetime(sp500_rp.future_ret_date)\n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rpid.merge(sp500_rp,left_on=[\"date\",\"rp_entity_id\"],right_on = [\"future_ret_date\",\"rp_entity_id\"],how = \"inner\")\n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rp_future_ret[[\"date\",\"rp_entity_id\",\"comnam\",\"CO_ret\",\"ret\",\"headline\",\"bool_CO_ret\"]]\n",
    "    \n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rp_future_ret.drop_duplicates(subset = [\"rp_entity_id\",\"headline\"])\n",
    "    sp500_crsp_rp_future_ret[\"future_ret\"] = sp500_crsp_rp_future_ret.apply(lambda row: row[\"CO_ret\"] if row[\"bool_CO_ret\"] \\\n",
    "                                                                            else (row[\"ret\"] if not row[\"bool_CO_ret\"]  else None), axis = 1)\n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rp_future_ret.drop(columns = ['CO_ret','ret','bool_CO_ret'])\n",
    "    # sp500_crsp_rp_future_ret.drop_duplicates(inplace = True)\n",
    "    sp500_crsp_rp_future_ret.dropna(inplace = True)\n",
    "    \n",
    "    return sp500_crsp_rp_future_ret\n",
    "\n",
    "# Remove individual commenter first\n",
    "def remove_individual(headline):\n",
    "    by_pattern = re.search(r\" (-+)? By|-[0-9]+-|- \\b[A-Z][a-z]{2}\\b \\d{1,2}\", headline, flags=re.IGNORECASE)\n",
    "    if by_pattern:\n",
    "        return headline[:by_pattern.start()]\n",
    "    else:\n",
    "        return headline\n",
    "    \n",
    "# Tokenize the headline into tokens which are alphanumeric words including period . \n",
    "def custom_tokenizer(headline):\n",
    "    tokens = re.findall(r\"\\b[a-zA-z0-9\\.][a-zA-z0-9\\.]+\\b\",headline.lower())  \n",
    "    return tokens\n",
    "\n",
    "# Remove tokens according to the principles \n",
    "def custom_processor(headline, remove_words):\n",
    "    tokens = custom_tokenizer(headline)\n",
    "    new_tokens = [token for token in tokens if token not in remove_words]\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "def clean_heandline(df):\n",
    "    df[\"vocab_con_headline\"] = df.headline.apply(remove_individual)\n",
    "    vocab_con_headlines = df.vocab_con_headline.tolist()\n",
    "    vocab = collections.Counter()\n",
    "\n",
    "    # Create a vocab dictionary with the words and their counts\n",
    "    for headline in vocab_con_headlines:\n",
    "        vocab.update(custom_tokenizer(headline))\n",
    "    remove_words = list(ENGLISH_STOP_WORDS)\n",
    "    words_once = [word for word,count in vocab.items() if count ==1]\n",
    "    top_100_count = sorted(vocab.values(),reverse = True)[99]\n",
    "    words_top_100 = [word for word,count in vocab.items() if count >=top_100_count]\n",
    "    remove_words.extend(words_once)\n",
    "    remove_words.extend(words_top_100)\n",
    "    remove_words = set(remove_words)\n",
    "\n",
    "    df.vocab_con_headline = df.vocab_con_headline.apply(lambda headline: custom_processor(headline, remove_words))\n",
    "    df_cleaned = df.dropna()\n",
    "    df_cleaned = df_cleaned[df_cleaned.vocab_con_headline != \"\"]\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 43727/43727 [06:30<00:00, 112.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 43617/43617 [06:22<00:00, 114.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 43565/43565 [06:52<00:00, 105.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 44894/44894 [07:45<00:00, 96.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 43486/43486 [07:12<00:00, 100.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 43613/43613 [07:16<00:00, 99.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 47231/47231 [08:25<00:00, 93.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51601/51601 [08:49<00:00, 97.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 56083/56083 [08:55<00:00, 104.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 54658/54658 [09:20<00:00, 97.52it/s] \n"
     ]
    }
   ],
   "source": [
    "df_folder = '/shared/share_tm-finance'\n",
    "\n",
    "for i in range(2014,2024):\n",
    "    datatype = \"contem\"\n",
    "    year_range = (i, i)\n",
    "    if datatype == \"contem\":\n",
    "        df = contem_ret(year_range)\n",
    "    else:\n",
    "        df = future_ret(year_range)\n",
    "\n",
    "    # save processed datafram\n",
    "    df_cleaned = clean_heandline(df)\n",
    "    print(\"cleaned \", i)\n",
    "    df_cleaned.to_csv(df_folder+\"/Processed_df_Sentiment/One_year_window/{type}_{year}_senti.csv\".format(type = datatype, year = i), index = False)\n",
    "\n",
    "    # save embeddings\n",
    "    red_headlines = df_cleaned.vocab_con_headline.tolist()\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedding_model.encode(red_headlines, show_progress_bar = True)\n",
    "    embeddings = np.save(df_folder+\"/Embeddings_with_Sentiment/One_year_window/{type}_{year}_senti_embeddings.npy\".format(type = datatype, year = i), embeddings)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_folder = '/shared/share_tm-finance'\n",
    "# datatype = \"contem\"\n",
    "# i = 2014\n",
    "# df_cleaned_grid=pd.read_csv(df_folder+\"/Processed_df_Sentiment/One_year_window/{type}_{year}_senti.csv\".format(type = datatype, year = i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
