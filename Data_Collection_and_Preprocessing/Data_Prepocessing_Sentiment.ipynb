{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "import datetime as dt\n",
    "import pandas_market_calendars as mcal\n",
    "import re\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "db = wrds.Connection(wrds_username = \"kevinlin5549\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SP500_CRSP_data(year_range):\n",
    "    first_year, last_year = str(year_range[0]),str(year_range[1])\n",
    "    sp_500_query = f\"\"\"SELECT a.*, b.date, b.ret, b.prc, b.openprc\n",
    "                        FROM crsp.dsp500list as a,\n",
    "                        crsp.dsf as b\n",
    "                        WHERE a.permno=b.permno\n",
    "                        and b.date >= a.start and b.date<= a.ending\n",
    "                        and b.date>='01/01/{first_year}' and b.date<='12/31/{last_year}'\n",
    "                        order by date;\"\"\"\n",
    "    sp_500 = db.raw_sql(sp_500_query,date_cols=['start', 'ending', 'date'])\n",
    "    dse = db.raw_sql(\"\"\"\n",
    "                        select comnam,ncusip, namedt, nameendt,permno\n",
    "                        from crsp.dsenames\n",
    "                        \"\"\", date_cols=['namedt', 'nameendt'])\n",
    "    dse['nameendt']=dse['nameendt'].fillna(pd.to_datetime('today'))\n",
    "    sp500_full = pd.merge(sp_500, dse, how = 'left', on = 'permno')\n",
    "    sp500_full = sp500_full.loc[(sp500_full.date>=sp500_full.namedt) \\\n",
    "                                & (sp500_full.date<=sp500_full.nameendt)]\n",
    "    sp500_full.reset_index(inplace = True,drop = True)\n",
    "    sp500_full = sp500_full[[\"permno\",\"date\",\"comnam\",\"ret\",\"openprc\",\"prc\"]]\n",
    "    sp500_full[\"prc\"] = sp500_full[\"prc\"].apply(abs)\n",
    "    sp500_full[\"CO_ret\"] = (sp500_full['prc'] - sp500_full['openprc'])/sp500_full['openprc']\n",
    "    return sp500_full\n",
    "\n",
    "mapping_file = pd.read_csv(\"SP500_Mapping_file.csv\")\n",
    "SP500_entity_id_str = ','.join(f\"'{id}'\" for id in list(mapping_file.rp_entity_id)) \n",
    "mapping_file.head()\n",
    "\n",
    "def SP500_RP_headline_data(year_range):\n",
    "    years = range(int(year_range[0]),int(year_range[-1])+1)\n",
    "    years_str = [str(year) for year in years]\n",
    "    RP_df = pd.DataFrame()\n",
    "    for year in years_str:\n",
    "        RP_year_query =f\"\"\"\n",
    "                        SELECT DISTINCT timestamp_utc,rp_entity_id,headline,css,relevance\n",
    "                        FROM rpna.rpa_djpr_equities_{year}\n",
    "                        WHERE rp_entity_id IN ({SP500_entity_id_str})\n",
    "                        \"\"\"\n",
    "        RP_df = pd.concat((RP_df,db.raw_sql(RP_year_query)),axis = 0)\n",
    "    RP_df = RP_df.drop_duplicates((\"rp_entity_id\",\"headline\"))\n",
    "    return RP_df\n",
    "\n",
    "def contem_ret(year_range):\n",
    "    first_year,last_year = int(year_range[0]),int(year_range[-1])\n",
    "\n",
    "    # Create financial dataframe from crsp and link to entity id\n",
    "    sp500_crsp = SP500_CRSP_data(year_range)\n",
    "    sp500_crsp_rpid =  sp500_crsp.merge(mapping_file, on = \"permno\", how = \"inner\")\n",
    "\n",
    "    # Create RavenPack headline data and map the timestamp to contemporaneous return date\n",
    "    sp500_rp =  SP500_RP_headline_data(year_range)\n",
    "    sp500_rp.set_index(\"timestamp_utc\",inplace= True)\n",
    "    sp500_rp[\"timestamp_NY\"] = pd.to_datetime(sp500_rp.index).tz_localize(\"UTC\").tz_convert(\"America/New_York\")\n",
    "    sp500_rp = sp500_rp.reset_index()\n",
    "    sp500_rp['index'] = sp500_rp.index\n",
    "    nyse = mcal.get_calendar(\"NYSE\")\n",
    "    nyse_tradingdays= nyse.valid_days(start_date=f\"{str(first_year)}-01-01\",end_date=f\"{str(last_year)}-12-31\")\\\n",
    "        .tz_localize(None).tz_localize(\"America/New_York\")\n",
    "    nyse_tradingdays_closing = nyse_tradingdays + dt.timedelta(hours = 16)\n",
    "    trading_days_df = pd.DataFrame({'trading_close': nyse_tradingdays_closing, 'contem_ret_date': nyse_tradingdays_closing.date})\n",
    "\n",
    "    # Use merge_asof to align the headlines with the trading close times\n",
    "    sp500_rp = pd.merge_asof(sp500_rp.sort_values('timestamp_NY'), trading_days_df,\n",
    "                            left_on='timestamp_NY', right_on='trading_close',\n",
    "                            direction='forward')\n",
    "    sp500_rp = sp500_rp.sort_values(\"index\")\n",
    "    sp500_rp\n",
    "    sp500_rp = sp500_rp[['timestamp_utc', 'rp_entity_id', 'headline', 'css', 'relevance', 'timestamp_NY', 'contem_ret_date']]\n",
    "    sp500_rp = sp500_rp.reset_index()\n",
    "    sp500_rp.drop(columns = [\"index\"],inplace = True)\n",
    "\n",
    "    # Merge crsp dataframe with RP dataframe\n",
    "    sp500_rp_contem_ret = sp500_rp[[\"contem_ret_date\",\"rp_entity_id\",\"headline\", \"css\", \"relevance\"]]\n",
    "    sp500_rp_contem_ret = sp500_rp_contem_ret.dropna()\n",
    "    sp500_rp_contem_ret.contem_ret_date = pd.to_datetime(sp500_rp_contem_ret.contem_ret_date)\n",
    "    sp500_crsp_rpid = sp500_crsp_rpid[[\"date\",\"rp_entity_id\",\"comnam\",\"ret\"]]\n",
    "    sp500_crsp_rp_contem_ret = pd.merge(sp500_crsp_rpid,sp500_rp_contem_ret,left_on=[\"date\",\"rp_entity_id\"],\\\n",
    "                                        right_on=[\"contem_ret_date\",\"rp_entity_id\"],how = \"inner\").drop(columns = \"contem_ret_date\")\n",
    "    \n",
    "    # sp500_crsp_rp_contem_ret.drop_duplicates((\"rp_entity_id\",\"headline\"),inplace=True)\n",
    "    sp500_crsp_rp_contem_ret.dropna(inplace=True)\n",
    "    return sp500_crsp_rp_contem_ret\n",
    "\n",
    "def future_ret(year_range):\n",
    "    first_year,last_year = int(year_range[0]),int(year_range[-1])\n",
    "\n",
    "    # Create financial dataframe from crsp and link to entity id\n",
    "    sp500_crsp = SP500_CRSP_data(year_range)\n",
    "    sp500_crsp_rpid =  sp500_crsp.merge(mapping_file, on = \"permno\", how = \"inner\")\n",
    "\n",
    "    # Create RavenPack headline data and map the timestamp to future return date\n",
    "    sp500_rp =  SP500_RP_headline_data(year_range)\n",
    "    sp500_rp.set_index(\"timestamp_utc\",inplace= True)\n",
    "    sp500_rp[\"timestamp_NY\"] = pd.to_datetime(sp500_rp.index).tz_localize(\"UTC\").tz_convert(\"America/New_York\")\n",
    "    nyse = mcal.get_calendar(\"NYSE\")\n",
    "    nyse_tradingdays= nyse.valid_days(start_date=f\"{str(first_year)}-01-01\",end_date=f\"{str(last_year)}-12-31\")\\\n",
    "        .tz_localize(None).tz_localize(\"America/New_York\")\n",
    "    nyse_tradingdays_opening = nyse_tradingdays + dt.timedelta(hours = 9)\n",
    "    nyse_tradingdays_closing = nyse_tradingdays + dt.timedelta(hours = 16)\n",
    "    def future_ret_date(timestamp):\n",
    "        later_opening = nyse_tradingdays_opening[nyse_tradingdays_opening>=timestamp]\n",
    "        later_closing = nyse_tradingdays_closing[nyse_tradingdays_closing>=timestamp]\n",
    "        if (not later_opening.empty) & (not later_closing.empty):\n",
    "            next_opening = later_opening[0]\n",
    "            next_closing = later_closing[0]\n",
    "            if next_opening.date() == next_closing.date():\n",
    "                return [next_opening.date(),1]\n",
    "            else: \n",
    "                return [next_opening.date(),0]\n",
    "        else:\n",
    "            return [None,None]\n",
    "    sp500_rp_future_ret_date = sp500_rp.apply(lambda row:future_ret_date(row['timestamp_NY']),axis = 1, result_type=\"expand\")\n",
    "    sp500_rp_future_ret_date = sp500_rp_future_ret_date.rename(columns = {0:\"future_ret_date\",1:\"bool_CO_ret\"}) \n",
    "    sp500_rp = pd.concat([sp500_rp,sp500_rp_future_ret_date],axis = 1)\n",
    "\n",
    "    # Merge crsp dataframe with RP dataframe\n",
    "    sp500_rp.future_ret_date = pd.to_datetime(sp500_rp.future_ret_date)\n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rpid.merge(sp500_rp,left_on=[\"date\",\"rp_entity_id\"],right_on = [\"future_ret_date\",\"rp_entity_id\"],how = \"inner\")\n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rp_future_ret[[\"date\",\"rp_entity_id\",\"comnam\",\"CO_ret\",\"ret\",\"headline\",\"bool_CO_ret\", \"relevance\"]]\n",
    "    \n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rp_future_ret.drop_duplicates(subset = [\"rp_entity_id\",\"headline\"])\n",
    "    sp500_crsp_rp_future_ret[\"future_ret\"] = sp500_crsp_rp_future_ret.apply(lambda row: row[\"CO_ret\"] if row[\"bool_CO_ret\"] \\\n",
    "                                                                            else (row[\"ret\"] if not row[\"bool_CO_ret\"]  else None), axis = 1)\n",
    "    sp500_crsp_rp_future_ret = sp500_crsp_rp_future_ret.drop(columns = ['CO_ret','ret','bool_CO_ret'])\n",
    "    # sp500_crsp_rp_future_ret.drop_duplicates(inplace = True)\n",
    "    sp500_crsp_rp_future_ret.dropna(inplace = True)\n",
    "    \n",
    "    return sp500_crsp_rp_future_ret\n",
    "\n",
    "# Remove individual commenter first\n",
    "def remove_useless(headline):\n",
    "    end_pattern = re.search(r\" (-+)? By|-[0-9]+-|- ?\\b[A-Z][a-z]{2}\\b \\d{1,2}|(null)|--?\\s\\w{1,}(\\s\\w{1,})?$|-- Barrons.com|researchandmarkets.com|by\\s\\w{1,}(\\s\\w{1,})?$|>[A-Z]{2,4}\", headline, flags=re.IGNORECASE)\n",
    "    if end_pattern:\n",
    "        headline = headline[:end_pattern.start()]\n",
    "    return headline\n",
    "\n",
    "\n",
    "def remove_number_day(headline):\n",
    "    headline = re.sub(r'\\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\b', '', headline, flags=re.IGNORECASE)\n",
    "    headline = re.sub(r'\\bMay\\b', '', headline)\n",
    "    headline = re.sub(r'\\b(?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)\\b', '', headline, flags=re.IGNORECASE)\n",
    "    headline = re.sub(r'\\b(Sunday|sunday|Sun)\\b', '', headline)\n",
    "    headline = re.sub(r'\\b(P.M.|A.M.|PM|AM|p.m.|a.m.|pm|am)\\b', '', headline)\n",
    "    headline = re.sub(r\"\\d*\\.?\\d+\\s*[Bb]+\\s|\\d*\\.?\\d+\\s*bn+\\s\", 'bln ', headline)\n",
    "    headline = re.sub(r\"\\d*\\.?\\d+\\s*[Mm]+\\s|\\d*\\.?\\d+\\s*mn+\\s\", 'mln ', headline)\n",
    "    headline = re.sub(r\"\\d*\\.?\\d+\\s*[Pp]ercent(?:age)?\", '', headline)\n",
    "    headline = re.sub(r\"\\d*\\.?\\d+[Kk]\", 'k', headline)\n",
    "    headline = re.sub(r'\\b\\d+(\\.\\d+)', '', headline)\n",
    "    return headline\n",
    "\n",
    "\n",
    "# Tokenize the headline into tokens which are alphanumeric words including period . \n",
    "def custom_tokenizer(headline):\n",
    "    tokens = re.findall(r\"\\b[a-zA-z0-9\\.][a-zA-z0-9\\.]+\\b\",headline.lower())  \n",
    "    return tokens\n",
    "\n",
    "# Remove tokens according to the principles \n",
    "def custom_processor(headline, remove_words):\n",
    "    tokens = custom_tokenizer(headline)\n",
    "    new_tokens = [token for token in tokens if token not in remove_words]\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "def get_user_selected_words(words_top, bool):\n",
    "    if bool:\n",
    "        print(\"Top words in headlines:\")\n",
    "        for word in words_top:\n",
    "            print(word)\n",
    "        print(\"\\nPlease enter the words you want to delete, separated by commas:\")\n",
    "        user_input = input().strip()\n",
    "        selected_words = [word.strip() for word in user_input.split(\",\") if word.strip() in words_top]\n",
    "    else:\n",
    "        selected_words = []\n",
    "    return selected_words\n",
    "\n",
    "def clean_heandline(df, relevance_threshold, top_words_num, select_manually = True):\n",
    "    headline_list = []\n",
    "    word_list = []\n",
    "    headline_list.append(len(df[\"headline\"]))\n",
    "    word_list.append(df[\"headline\"].apply(lambda x: len(x.split())).sum())\n",
    "    \n",
    "\n",
    "    # remove headlines with a relevance score smaller than 75 (recommended by RavenPack)\n",
    "    df = df[df['relevance'] >= relevance_threshold]\n",
    "    headline_list.append(len(df[\"headline\"]))\n",
    "    word_list.append(df[\"headline\"].apply(lambda x: len(x.split())).sum())\n",
    "\n",
    "\n",
    "    # replace numbers with magnitudes and remove all numbers\n",
    "    df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"headline\"].apply(remove_number_day)\n",
    "    headline_list.append(len(df[\"vocab_con_headline\"]))\n",
    "    word_list.append(df[\"vocab_con_headline\"].apply(lambda x: len(x.split())).sum())\n",
    "\n",
    "\n",
    "    # remove irrelevant information\n",
    "    stop_words = list(ENGLISH_STOP_WORDS)\n",
    "    words_list = [\"release\", \"press\", \"pgr\", \"mw\", \"llp\", \"corp\", \"live\", \"corporation\",\"plc\", \"factset\",\n",
    "                     \"llc\", \"group\", \"target\", \"blog\", \"st\", \"chart\", \"update\", \"dir\", \"barron\", \"pbulletin\"]\n",
    "    stop_words.extend(words_list)\n",
    "    #remove news sources\n",
    "    df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"vocab_con_headline\"].apply(remove_useless)\n",
    "    df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"vocab_con_headline\"].apply(lambda headline: custom_processor(headline, stop_words))\n",
    "    headline_list.append(len(df[\"vocab_con_headline\"]))\n",
    "    word_list.append(df[\"vocab_con_headline\"].apply(lambda x: len(x.split())).sum())\n",
    "\n",
    "\n",
    "    # Remove words that appear only once and the top 150 words\n",
    "    remove_words = set()\n",
    "    # Create a vocab dictionary with the words and their counts\n",
    "    vocab_con_headlines = df.loc[:,\"vocab_con_headline\"].tolist()\n",
    "    vocab = collections.Counter()\n",
    "    for headline in vocab_con_headlines:\n",
    "        vocab.update(custom_tokenizer(headline))\n",
    "\n",
    "    words_once = [word for word,count in vocab.items() if count ==1]\n",
    "    remove_words.update(words_once)\n",
    "    top_count = sorted(vocab.values(),reverse = True)[top_words_num]\n",
    "    words_top = [word for word,count in vocab.items() if count >=top_count]\n",
    "    \n",
    "    user_selected_words = get_user_selected_words(words_top, select_manually)\n",
    "    remove_words.update(user_selected_words)\n",
    "    df.loc[:,\"vocab_con_headline\"] = df.loc[:,\"vocab_con_headline\"].apply(lambda headline: custom_processor(headline, remove_words))\n",
    "    headline_list.append(len(df[\"vocab_con_headline\"]))\n",
    "    word_list.append(df[\"vocab_con_headline\"].apply(lambda x: len(x.split())).sum())\n",
    "\n",
    "\n",
    "    # remove empty headlines\n",
    "    df = df.drop(columns = [\"relevance\"])\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df.vocab_con_headline != \"\"]\n",
    "    headline_list.append(len(df[\"vocab_con_headline\"]))\n",
    "    word_list.append(df[\"vocab_con_headline\"].apply(lambda x: len(x.split())).sum())\n",
    "    \n",
    "    return df, headline_list, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10222/10222 [01:22<00:00, 124.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9740/9740 [01:18<00:00, 123.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9936/9936 [01:17<00:00, 127.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10275/10275 [01:22<00:00, 124.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10076/10076 [01:21<00:00, 123.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 10612/10612 [01:24<00:00, 125.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12220/12220 [01:39<00:00, 122.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 11986/11986 [01:37<00:00, 123.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12147/12147 [01:37<00:00, 123.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned  2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12769/12769 [01:40<00:00, 126.56it/s]\n"
     ]
    }
   ],
   "source": [
    "df_folder = '/shared/share_tm-finance/Final'\n",
    "datatype = \"future\"\n",
    "relevance_threshold = 75\n",
    "top_words_num = 150\n",
    "year_num = 1\n",
    "select_manually = False\n",
    "\n",
    "\n",
    "for i in range(2014,2024):\n",
    "    year_range = (i - year_num + 1, i)\n",
    "    if datatype == \"contem\":\n",
    "        df = contem_ret(year_range)\n",
    "    else:\n",
    "        df = future_ret(year_range)\n",
    "\n",
    "    \n",
    "    # save processed datafram\n",
    "    df_cleaned, headline_list, word_list = clean_heandline(df, relevance_threshold, top_words_num, select_manually)\n",
    "\n",
    "    file_path = 'headline number.xlsx'\n",
    "    wb = openpyxl.load_workbook(file_path)\n",
    "\n",
    "    ws = wb[f'{datatype}_{year_num}']\n",
    "    for cell in ws['A']:\n",
    "        if cell.value == 'headlines':\n",
    "            j = 0\n",
    "            for row_num in range(cell.row + 1, cell.row + 7):\n",
    "                ws.cell(row=row_num, column=i - 2013 + 1, value=headline_list[j])\n",
    "                j += 1\n",
    "        elif cell.value == 'words':\n",
    "            j = 0\n",
    "            for row_num in range(cell.row + 1, cell.row + 7):\n",
    "                ws.cell(row=row_num, column=i - 2013 + 1, value=word_list[j])\n",
    "                j += 1\n",
    "    wb.save(file_path)\n",
    "\n",
    "    print(\"cleaned \", i)\n",
    "    df_cleaned.to_csv(df_folder+f\"/Processed_df/{year_num}_year_window/{datatype}_{i - year_num + 1}_{i}.csv\", index = False)\n",
    "\n",
    "    #save embeddings\n",
    "    red_headlines = df_cleaned.vocab_con_headline.tolist()\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = embedding_model.encode(red_headlines, show_progress_bar = True)\n",
    "    embeddings = np.save(df_folder+f\"/Embeddings/{year_num}_year_window/{datatype}_{i - year_num + 1}_{i}_embeddings.npy\", embeddings)\n",
    "    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
